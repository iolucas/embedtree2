{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedtree2\n",
    "Notebook to compile the so far research of nvgtt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from urllib.parse import quote, unquote\n",
    "import networkx as nx\n",
    "import sys\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from ThreadPool import ThreadPool\n",
    "\n",
    "from wikipydia import dict_storage, wikidb\n",
    "DictStorage = dict_storage.DictStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_db = wikidb.WikiDb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_percent_reported = None\n",
    "def download_progress_hook(count, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wikisyn = DictStorage(\"wikisyn\") #Storage for link synoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiSynBeta:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename + \".pickle\"\n",
    "        \n",
    "        self.submitted_pageids = set()\n",
    "        self.hrefs = dict()\n",
    "        \n",
    "        try:\n",
    "            with open(self.filename, mode='r+b') as pickle_file:\n",
    "                saved_data = pickle.load(pickle_file)\n",
    "                \n",
    "                self.submitted_pageids = saved_data.submitted_pageids\n",
    "                self.hrefs = saved_data.hrefs\n",
    "                \n",
    "        except IOError:\n",
    "            print(\"Failed to open \" + self.filename + \". Created empty wikisyn.\")\n",
    "        \n",
    "    def save(self):\n",
    "        with open(self.filename, mode='w+b') as pickle_file:\n",
    "            pickle.dump(self, pickle_file)\n",
    "    \n",
    "    def submit_article(self, wikiart):\n",
    "        if wikiart.page_id in self.submitted_pageids:\n",
    "            return False\n",
    "        \n",
    "        for link_href, link_text in wikiart.links():\n",
    "            #If the link_text is invalid (empty, spaces etc) skip it\n",
    "            if not link_text:\n",
    "                continue\n",
    "            \n",
    "            #Ensure link_text is lower case to compute it only once\n",
    "            link_text = link_text.lower()\n",
    "            \n",
    "            #Init this href if it has not been initiated\n",
    "            if link_href not in self.hrefs:\n",
    "                self.hrefs[link_href] = dict()\n",
    "            \n",
    "            #Init this href text if it has not been initiated   \n",
    "            if link_text not in self.hrefs[link_href]:\n",
    "                self.hrefs[link_href][link_text] = 0\n",
    "                \n",
    "            self.hrefs[link_href][link_text] += 1 #Add the occurence of this text in this href\n",
    "            \n",
    "        self.submitted_pageids.add(wikiart.page_id)\n",
    "        return True\n",
    "             \n",
    "    def get_synoms(self, href, norm=True):\n",
    "        \n",
    "        if href not in self.hrefs:\n",
    "            return list()\n",
    "        \n",
    "        if not norm:\n",
    "            return self.hrefs[href].items()\n",
    "        \n",
    "        norm_fact = 0\n",
    "        for text, score in self.hrefs[href].items():\n",
    "            norm_fact += score\n",
    "            \n",
    "        norm_synoms = list()\n",
    "        for text, score in self.hrefs[href].items():\n",
    "            norm_synoms.append((text, score / norm_fact))\n",
    "        \n",
    "        return norm_synoms\n",
    "        \n",
    "        #synoms = list()\n",
    "        \n",
    "        #norm_fact = 0\n",
    "        \n",
    "\n",
    "        \n",
    "        #for l_text, l_score in self.hrefs[href].items():\n",
    "            #norm_fact += l_score  \n",
    "        \n",
    "        #for link_text in self.hrefs[href]:\n",
    "            #synoms.append(link_text.items())\n",
    "        \n",
    "        #return synoms\n",
    "    \n",
    "    def get_joined_synoms(self, page_hrefs, norm=True):\n",
    "        \n",
    "        synoms = dict()\n",
    "        \n",
    "        norm_fact = 0\n",
    "        \n",
    "        for href in page_hrefs:\n",
    "            if href not in self.hrefs:\n",
    "                continue\n",
    "            for l_text, l_score in self.hrefs[href].items():\n",
    "                \n",
    "                if l_text not in synoms:\n",
    "                    synoms[l_text] = 0\n",
    "                \n",
    "                synoms[l_text] += l_score\n",
    "                norm_fact += l_score                \n",
    "        \n",
    "        #If we should not return scores normalized\n",
    "        if not norm:\n",
    "            return synoms.items()\n",
    "        \n",
    "        norm_synoms = list()\n",
    "        for text, score in synoms.items():\n",
    "            norm_synoms.append((text, score / norm_fact))\n",
    "        \n",
    "        return norm_synoms\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synbeta = WikiSynBeta(\"wikisynbeta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_wikisynbeta():\n",
    "    \n",
    "    syntest = WikiSynBeta(\"syntest\")\n",
    "    \n",
    "    wikiart, _ = wiki_db.get_article_by_href(\"Node.js\")\n",
    "    print(syntest.submit_article(wikiart)) \n",
    "    print(syntest.submit_article(wikiart)) \n",
    "    \n",
    "    print(syntest.submitted_pageids)\n",
    "    #for href in synbeta.get_joined_synoms([\"JavaScript\"], False):\n",
    "    for href in syntest.get_synoms(\"Angular_(application_platform)\"):\n",
    "        print(href)\n",
    "    \n",
    "    for h in syntest.hrefs.items():\n",
    "        print(h)\n",
    "        \n",
    "def test_wikisynsave():\n",
    "    syntest = WikiSynBeta(\"syntest\")\n",
    "    print(syntest.submitted_pageids)\n",
    "    print(syntest.hrefs)\n",
    "    \n",
    "    wikiart, _ = wiki_db.get_article_by_href(\"Node.js\")\n",
    "    synbeta.submit_article(wikiart) \n",
    "    \n",
    "    print(synbeta.submitted_pageids)\n",
    "    print(synbeta.hrefs)\n",
    "    \n",
    "    synbeta.save()\n",
    "   \n",
    "#test_wikisynbeta()\n",
    "\n",
    "#test_wikisynsave()\n",
    "\n",
    "#synbeta = WikiSynBeta()\n",
    "#test_wikisynbeta()\n",
    "#test_wikisynbeta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_article_by_href(href):\n",
    "    wikiart, downloaded = wiki_db.get_article_by_href(href)\n",
    "    #Populate wikisyn\n",
    "    synbeta.submit_article(wikiart)\n",
    "    \n",
    "    #for link_href, link_text in wikiart.links():\n",
    "        #if not link_href in wikisyn:\n",
    "            #wikisyn[link_href] = set()\n",
    "        #wikisyn[link_href].add(link_text)\n",
    "        \n",
    "    return wikiart\n",
    "\n",
    "def get_wiki_article_by_title(title):\n",
    "    return get_wiki_article_by_href(quote(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_wikisyns_by_href(href):\n",
    "    wikiart = get_wiki_article_by_href(href)\n",
    "    hrefs = wiki_db.pageid_to_href[wikiart.page_id]\n",
    "    \n",
    "    wikisyns = set()\n",
    "    for page_href in hrefs:\n",
    "        if page_href not in wikisyn:\n",
    "            continue\n",
    "        for syn in wikisyn[page_href]:\n",
    "            if syn: #CHeck if the text is empty\n",
    "                wikisyns.add(syn.lower()) \n",
    "    \n",
    "    return wikisyns\n",
    "\n",
    "def get_all_wikisyns_by_title(title):\n",
    "    return get_all_wikisyns_by_href(quote(title))\n",
    "\n",
    "#print(get_all_page_wikisyns(\"javascript\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_everything():\n",
    "    wiki_db.save()\n",
    "    synbeta.save()\n",
    "    #wikisyn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_get_wiki_article_by_href(href):\n",
    "    wikiart = get_wiki_article_by_href(href)\n",
    "    save_everything()\n",
    "    return wikiart\n",
    "\n",
    "#print(test_get_wiki_article_by_href(\"c%2b%2b\").title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to download bunch of wikipedia pages at once if they are not present\n",
    "check_and_download__done = 0\n",
    "def check_and_download(pages):\n",
    "    global check_and_download__done\n",
    "    check_and_download__done = 0\n",
    "    \n",
    "    n_tasks = len(pages)\n",
    "    \n",
    "    print(\"Tasks to go: \" + str(n_tasks))\n",
    "    \n",
    "    # Function to be executed in a thread\n",
    "    def download_stuff(page):\n",
    "        global check_and_download__done\n",
    "        #try:\n",
    "        get_wiki_article_by_href(page)\n",
    "        #except:\n",
    "            #print(\"Failed to get page \" + page + \". Timed out.\")\n",
    "        #finally:\n",
    "        check_and_download__done += 1\n",
    "        download_progress_hook(check_and_download__done, n_tasks)\n",
    "            #percentage = round(check_and_download__done * 100 / n_tasks)\n",
    "            #if percentage % 5 == 0:\n",
    "                #print(\"{0}%.....\".format(percentage), end=\"\")\n",
    "            #print(\"Done \" + str(check_and_download__done) + \"/\" + str(n_tasks))\n",
    "\n",
    "    # Instantiate a thread pool with 5 worker threads\n",
    "    pool = ThreadPool(50)\n",
    "\n",
    "    pool.map(download_stuff, pages)\n",
    "    pool.wait_completion()\n",
    "    \n",
    "    print(\"\\nFinishing downloading. Done tasks: \" + str(check_and_download__done) + \"/\" + str(n_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check_and_download([\"Node.js\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#target_page = urllib.quote(\"JavaScript\")\n",
    "#target_id = get_pageid(target_page)\n",
    "#target_links = pageid_to_page_links[target_id]\n",
    "#target_links_ids = set()\n",
    "\n",
    "#for i, link in enumerate(target_links):\n",
    "    #print(\"Working on \" + link + \". \" + str((i+1)) + \"/\" + str(len(target_links)))\n",
    "    #target_links_ids.add(get_pageid(link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_sorted_list(data, key, reverse=False):\n",
    "    for k, v in sorted(data, key=key, reverse=reverse):\n",
    "        print(k,v)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArXiv {'arxiv': 308} 0.053275468476275255\n",
      "Artificial_neural_network {'neural networks': 10, 'artificial neural network': 22, 'artificial neural networks': 13, 'neural network': 2} 0.050341650437531686\n",
      "Overfitting {'overfitting': 35, 'overfitted': 1} 0.0457019950164126\n",
      "Digital_object_identifier {'doi': 6141, 'digital object identifier': 6, 'doi number': 1} 0.04382396118086502\n",
      "C_(programming_language) {'c': 343, 'c programming language': 44, 'c language': 14, 'c-language': 2, 'standard c': 1, 'c (programming language)': 4, 'c programs': 1, 'c programming languages': 1, 'c-style': 1} 0.041845680692399974\n",
      "Convolution {'convolution': 46, 'convolutional': 3, 'convolutions': 2, 'convolving': 3, 'convolved in the normal way': 1, 'discrete convolution': 1, 'convolved': 2, 'applications of convolution': 1} 0.038190490363351856\n",
      "Regularization_(mathematics) {'regularization': 19, 'regularizer': 2, 'regularization penalty': 1, 'regularizing': 1, 'regularization (mathematics)': 3, 'regularisation': 1, 'regularized': 2} 0.03749814921149597\n",
      "Dropout_(neural_networks) {'dropout': 3} 0.03447236195523693\n",
      "Receptive_fields {'receptive fields': 8} 0.03133851086839721\n",
      "Receptive_field {'receptive fields': 12, 'receptive field': 16, 'field': 1} 0.03058206405433245\n",
      "Image_classification {'image classification': 1, 'classification of images': 1, 'classification': 2} 0.026637734238137627\n",
      "Machine_learning {'machine learning': 118, 'learning': 7, 'learning algorithms': 1, 'machines': 1} 0.026329284328015606\n",
      "L1-norm {'l1 norm': 1, 'l': 1, '1': 1, '-norm': 1, 'l1-norm': 1} 0.02507080869471777\n",
      "Deep_learning {'deep learning': 39, 'deep learning#software libraries': 1, 'deep-learning frameworks': 1, 'deep convolutional neural net': 1, 'deep learning methods': 2} 0.024999584806380502\n",
      "Artificial_neuron {'artificial neurons': 4, 'threshold logic': 1, 'neurons': 4, 'units': 1, 'linear output': 1, 'artificial neuron': 3, 'neuron': 1, 'linear neurons': 3, \"neuron's\": 1} 0.02375129244762736\n",
      "International_Standard_Book_Number {'isbn': 9387, 'international standard book number (isbn)': 1, 'international standard book number': 3, 'international standard book numbers (isbns)': 1, 'international standard book numbers': 1} 0.021922944859485916\n",
      "Database {'database': 82, 'databases': 24, 'database software and technology': 1, 'view-update problem': 1, 'locking': 1, 'end-user database': 1, 'dbms': 1, 'database §\\xa0languages': 1} 0.02132137971582024\n",
      "PubMed_Identifier {'pmid': 2728} 0.018803106521038327\n",
      "Institute_of_Electrical_and_Electronics_Engineers {'ieee': 19, 'institute of electrical and electronics engineers': 29, 'institute of electrical and electronics engineers (ieee)': 3} 0.018311522036828173\n",
      "Natural_language_processing {'natural language processing': 48, 'natural language': 4, 'speech recognition': 1, 'parsing': 1, 'understanding human speech': 1, 'nlp': 2} 0.01753857011757668\n",
      "Computer_vision {'computer vision': 53, 'vision': 2, 'image recognition': 1, 'image and video recognition': 1, 'computer': 1} 0.01631763841768269\n",
      "Neocognitron {'neocognitron': 9} 0.015669255434198604\n",
      "Visual_field {'visual field': 12} 0.015669255434198604\n",
      "GPU {'gpu': 9, 'gpus': 10} 0.01352504153267669\n",
      "IEEE_Conference_on_Computer_Vision_and_Pattern_Recognition {'ieee conference on computer vision and pattern recognition': 1} 0.009401553260519164\n",
      "Drug_discovery {'drug discovery': 4} 0.009401553260519164\n",
      "Biological {'biological': 10} 0.009401553260519164\n",
      "Retina {'retina': 48, 'retinal': 2, 'retinas': 1} 0.00884852071578274\n",
      "Python_(programming_language) {'python': 281, 'python code': 1, 'python vm': 1, 'python (programming language)': 2, 'pythonic': 1, 'python programming language': 10, 'python implementations': 1, 'several experimental implementations': 1, \"python's\": 1, 'python.org': 1} 0.008806121554019617\n",
      "Visual_system {'visual': 2, 'visual system': 23, 'retina': 1, 'optic nerve': 1, 'optic tract': 1, 'visual cortex': 1, 'vision': 1, 'pathways': 1, 'visual pathways': 1, 'visualize': 1, 'sight': 1} 0.008111143989467514\n",
      "Translation_(geometry) {'translations': 7, 'translation': 7, 'translating': 1, 'translated': 2, 'translation (geometry)': 1} 0.007312319202626016\n",
      "MNIST_database {'mnist handwritten digits problem': 1, 'mnist': 1, 'mnist database': 4, 'handwritten digits': 1} 0.00716308819849079\n",
      "Per-comparison_error_rate {'error rate': 1} 0.006267702173679442\n",
      "Monte_Carlo_tree_search {'monte carlo tree search': 5} 0.006267702173679442\n",
      "Video_quality {'video quality': 3} 0.006267702173679442\n",
      "Early_stopping {'early stopping': 6} 0.006267702173679442\n",
      "AlphaGo {'alphago': 10} 0.006267702173679442\n",
      "Object_detection {'object detection': 2} 0.006267702173679442\n",
      "CiteSeerX {'citeseerx': 61, 'article': 1} 0.006267702173679442\n",
      "Theano_(software) {'theano': 6} 0.006267702173679442\n",
      "GNU_Go {'gnu go': 2} 0.006267702173679442\n",
      "MATLAB {'matlab': 48} 0.006267702173679442\n",
      "Google_Deepmind {'google deepmind': 1} 0.006267702173679442\n",
      "Kernel_(image_processing) {'kernels': 1, 'kernel': 1} 0.006267702173679442\n",
      "International_Standard_Serial_Number {'issn': 559, 'international standard serial number': 3} 0.006234244688766562\n",
      "C%2B%2B {'c++': 389, 'planned for july 2017': 1, 'c++ programming language': 2, 'c++98': 2} 0.006188162805993155\n",
      "Torch_(machine_learning) {'torch': 9, 'torch (machine learning)': 1} 0.005640931956311498\n",
      "Visual_cortex {'visual cortex': 30, 'visual': 1, 'v1': 1, 'primary visual cortex': 3, 'area v4': 1, 'middle temporal': 1, 'area v5': 1, 'v2': 1, 'v3': 1, 'v4': 1, 'mt': 1, 'striate cortex (v1)': 5, 'pathways': 2, 'layer 4': 1, 'prestriate cortex (v2)': 4, 'ventral extrastriate area (v4)': 3, 'dorsal extrastriate area (v5/mt)': 4, 'pathway': 4} 0.005544505769024121\n",
      "Hyperparameter_optimization {'sweeping through the parameter space': 1, 'hyperparameter': 1, 'hyperparameters': 2, 'hyperparameter optimization': 1} 0.0050141617389435535\n",
      "Multilayer_perceptron {'multilayer perceptrons': 5, 'multilayer perceptron': 7, 'mlp': 1} 0.004821309364368802\n",
      "Time_delay_neural_network {'time delay neural network': 4, 'time delay neural networks': 1} 0.0037606213042076656\n",
      "Mathematical_biology {'mathematical biology': 2, 'inspired': 1} 0.003133851086839721\n",
      "Orbital_hybridisation {'sp3 carbons': 1} 0.003133851086839721\n",
      "Yann_LeCun {'yann lecun': 14} 0.003133851086839721\n",
      "NumPy {'numpy': 12} 0.003133851086839721\n",
      "Root_mean_square_error {'root mean square error': 2} 0.003133851086839721\n",
      "Cortical_neuron {'cortical neurons': 1} 0.003133851086839721\n",
      "Multinomial_distribution {'multinomial distribution': 9, 'multinomial': 2} 0.003133851086839721\n",
      "Elastic_net_regularization {'elastic net regularization': 4, 'elastic net': 1} 0.003133851086839721\n",
      "RGB_images {'rgb images': 1} 0.003133851086839721\n",
      "ImageNet_Large_Scale_Visual_Recognition_Challenge {'imagenet large scale visual recognition challenge': 2} 0.003133851086839721\n",
      "TensorFlow {'tensorflow': 10} 0.003133851086839721\n",
      "Living_organisms {'living organisms': 1} 0.003133851086839721\n",
      "Hyperbolic_tangent {'hyperbolic tangent': 3} 0.003133851086839721\n",
      "Curse_of_dimensionality {'curse of dimensionality': 6} 0.003133851086839721\n",
      "Zero_norm {'zero norm': 1} 0.003133851086839721\n",
      "Proteins {'proteins': 5} 0.003133851086839721\n",
      "Simple_cells_(visual_cortex) {'simple cells': 1} 0.003133851086839721\n",
      "Scale_invariant_feature_transform {'scale invariant feature transform': 1} 0.003133851086839721\n",
      "Deeplearning4j {'deeplearning4j': 8} 0.003133851086839721\n",
      "Sigmoid_function {'sigmoid function': 6, 'sigmoid': 2, 'sigmoidal': 1, 'sigmoid shape': 1} 0.003133851086839721\n",
      "Electromyography {'electromyography': 4} 0.003133851086839721\n",
      "DeepDream {'deepdream': 9} 0.003133851086839721\n",
      "Complex_cells_(visual_cortex) {'complex cells': 1} 0.003133851086839721\n",
      "OCLC {'oclc': 397, 'online computer library center': 1} 0.0031259770891340936\n",
      "PubMed_Central {'pmc': 785, 'pubmed central': 10} 0.0030944315763134353\n",
      "Haskell_(programming_language) {'haskell': 90, 'haskell programming language': 1, 'haskell language': 1, 'haskell-like': 1} 0.0030327591162965044\n",
      "Lua_(programming_language) {'lua': 50, 'lua programming language': 1, 'luajit': 1} 0.003013318352730501\n",
      "Java_(programming_language) {'java': 380, 'java language': 4, 'java programming language': 14, 'java (programming language)': 4, 'stand-alone hello world application for java': 1, 'the java language': 1, \"java's\": 1} 0.0029404034888866515\n",
      "Scala_(programming_language) {'scala': 65, 'scala.js': 1, 'scala programming language': 2, 'java': 1, 'scala (programming language)': 1, 'for-expressions': 1} 0.0029131573483298814\n",
      "GPGPU {'gpgpu': 9, 'general-purpose gpu': 1} 0.002820465978155749\n",
      "Dot_product {'dot product': 63, 'matrix product': 1, 'dot products': 2, 'dot': 1, 'inner products': 1, 'scalar product': 1, 'inner product': 2, 'dotted': 1} 0.0027856454105241965\n",
      "Computer_Go {'computer go-playing system': 1, 'computer go': 4} 0.0025070808694717768\n",
      "L2_norm {'l2 norm': 4, '\\n\\n\\n\\n\\nl\\n\\n2\\n\\n\\n\\n\\n{\\\\displaystyle l^{2}}\\n\\n norm': 1} 0.0025070808694717768\n",
      "Translational_symmetry {'translation invariance': 1, 'translational symmetry': 4} 0.0025070808694717768\n",
      "Facial_recognition_system {'facial recognition technology': 1, 'facial recognition': 6, 'face recognition': 1, 'identifying a person from their face': 1} 0.0024374397342086717\n",
      "Vision_processing_unit {'vision processing unit': 7, 'vision processing units': 2} 0.0024374397342086717\n",
      "Activation_function {'activation function': 8, 'activation functions': 3} 0.0022791644267925243\n",
      "Recommender_system {'recommender systems': 2, 'recommender system': 1} 0.002089234057893147\n",
      "Apache_Spark {'apache spark': 2, 'spark': 3} 0.0018803106521038326\n",
      "Aromaticity {'aromaticity': 1, 'aromatic': 1} 0.0015669255434198606\n",
      "Rectifier_(neural_networks) {'rectified linear': 1, 'rectifier': 1, 'rectified linear units': 1, 'softplus function': 1} 0.0015669255434198606\n",
      "Data_pre-processing {'preprocessing': 1, 'data pre-processing': 1} 0.0015669255434198606\n",
      "Integer {'integers': 77, 'integer': 54, 'integral': 1, 'ring of integers': 1, 'whole numbers': 1, 'z': 1, 'integer-valued': 3, 'zahlen symbols': 1} 0.0012400130199725515\n",
      "Cross_entropy {'cross entropy': 2, 'cross-entropy': 1} 0.0010446170289465735\n",
      "Deterministic_algorithm {'deterministic algorithms': 1, 'determinism': 1, 'deterministic': 1} 0.0010446170289465735\n",
      "Precision_and_recall {'precision and recall': 5, 'precision': 2, 'f-measure': 1} 0.0007834627717099303\n",
      "Softmax_function {'softmax function': 3, 'softmax': 1} 0.0007834627717099303\n",
      "Sparse_approximation {'projected gradient descent': 1, 'sparse approximation': 4, 'sparse coding': 1} 0.0005223085144732868\n",
      "Go_ranks_and_ratings {'9 dan': 1, 'amateur dan': 1, '9 dan rank': 1, '9p': 1, '2 dan': 1, '6 dan': 1} 0.0005223085144732868\n",
      "Feedforward_neural_network {'feedforward': 3, 'feedforward neural network': 4, 'feedforward neural networks': 3, 'feedforward networks': 1, 'feed-forward': 1, 'single layer perceptron': 1} 0.0004821309364368802\n",
      "Euclidean_distance {'distance': 2, 'the distance': 1, 'euclidean distance': 12, 'usual distance': 1, 'length': 1, 'euclidean metric': 1, 'euclidean': 2} 0.0003133851086839721\n",
      "Hydrogen_bond {'hydrogen bonding': 1, 'hydrogen bonds': 9} 0.0003133851086839721\n"
     ]
    }
   ],
   "source": [
    "def get_links_score(page):\n",
    "    \"\"\"Function to cross a list of links with a text, setting scores.\"\"\"\n",
    "    \n",
    "    wikiart = get_wiki_article_by_href(page)\n",
    "    \n",
    "    pageid = wikiart.page_id\n",
    "    #Ensure only one href is present\n",
    "    page_links = set([link_href for link_href, link_text in wikiart.links()])\n",
    "    page_text = wikiart.text()\n",
    "    \n",
    "    links_score = dict()\n",
    "    \n",
    "    norm_fact = 0 #norm factor to results sum to one\n",
    "    \n",
    "    #Ensure all page links are present\n",
    "    #check_and_download(page_links)\n",
    "    \n",
    "    #The ideia is to get for each href the texts that use to follow these hrefs in wikipedia articles.\n",
    "    #The more each term appears, it got increased weight.\n",
    "    #Then we try to match each of the terms for each href to the wiki text, applying to each match the correspondent weight.\n",
    "    #The weight stuff is good to avoid cases where the href_text have appeared only once in one article,\n",
    "    #but it is a frequent term in other articles but offen does not mean the original href it pointed to\n",
    "    \n",
    "    for link_href in page_links:\n",
    "        links_score[link_href] = 0\n",
    "        for l_text, l_score in synbeta.get_synoms(link_href): #get_all_wikisyns_by_href(link_href): #wikisyn[link_href]:\n",
    "            matches = re.findall('[^a-zA-Z0-9_]' + re.escape(l_text) + '[^a-zA-Z0-9_]', page_text, re.IGNORECASE)\n",
    "            matches_score = len(matches) * l_score\n",
    "            links_score[link_href] += matches_score\n",
    "            norm_fact += matches_score\n",
    "            \n",
    "    norm_links_score = dict(map(lambda a: [a[0], float(a[1])/norm_fact], links_score.items()))\n",
    "            \n",
    "    return norm_links_score\n",
    "\n",
    "#v_sum = 0\n",
    "links_score = get_links_score(\"Convolutional neural network\")\n",
    "for k, v in sorted(links_score.items(), key=lambda a:a[1], reverse=True):\n",
    "    print(k,synbeta.hrefs[k],v)\n",
    "    #v_sum += v\n",
    "#print v_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_node_edges_scores(page_href, top=-1):\n",
    "    \"\"\"Function to get node edges to be placed in the graph. \"\"\"\n",
    "    \n",
    "    edges = dict()\n",
    "    \n",
    "    wikiart = get_wiki_article_by_href(page_href)\n",
    "    \n",
    "    #Get main page data\n",
    "    page_id = wikiart.page_id\n",
    "    page_title = wikiart.title\n",
    "    page_links = get_links_score(page_href).items()\n",
    "    \n",
    "    if top > -1:\n",
    "        sorted_page_links = sorted(page_links, key=lambda a: a[1], reverse=True)\n",
    "        page_links = sorted_page_links[:top]\n",
    "        #print(page_links)\n",
    "        \n",
    "    #Ensure everything has been downloaded first    \n",
    "    hrefsToDownload = [link_href for link_href, _ in page_links]        \n",
    "    check_and_download(hrefsToDownload)\n",
    "    \n",
    "    for i, (link_href, score) in enumerate(page_links):\n",
    "        \n",
    "        #print(\"Working on link {0} {1}/{2}\".format(link_href, i+1, len(page_links)))\n",
    "        \n",
    "        link_art = get_wiki_article_by_href(link_href)\n",
    "        \n",
    "        link_id = link_art.page_id\n",
    "        link_title = link_art.title\n",
    "        \n",
    "        #If there is already a title already place, sum the scores\n",
    "        if (page_title, link_title) in edges:\n",
    "            edges[(page_title, link_title)] += score\n",
    "        else:\n",
    "            edges[(page_title, link_title)] = score        \n",
    "        \n",
    "    return edges\n",
    "\n",
    "\n",
    "\n",
    "#print(get_links_score(\"JavaScript\"))\n",
    "\n",
    "#edges_scores = get_node_edges_scores(\"TensorFlow\")\n",
    "#print_sorted_list(edges_scores.items(), lambda a:a[1], True)\n",
    "\n",
    "#save_everything()\n",
    "\n",
    "#CREATE METHOD TO CREATE GRAPH BASED ON DEEPNESS --DONE\n",
    "#MAYBE PLACE STOP CONDITION TO NOT DOWNLOAD EVERY LINK --DONE\n",
    "#CHECK WHETHER WIKISYN IS REALLY GOOD BECAUSE OF ERRORS. MAYBE KEEP TRACK HOW MANY TIMES EACH WORD APPEARS DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_123(target, top=20):\n",
    "    \n",
    "    edges_scores = get_node_edges_scores(target, top)\n",
    "    print_sorted_list(edges_scores.items(), lambda a:a[1], True)\n",
    "    print(\"\\n\\n\")\n",
    "    links_score = get_links_score(target)\n",
    "    print_sorted_list(links_score.items(), lambda a:a[1], True)\n",
    "    \n",
    "#test_123(\"MQTT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(synbeta.hrefs[\"Publish%E2%80%93subscribe_pattern\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_graph_edges(seed_href, deepness, top=-1):\n",
    "    if deepness > 3:\n",
    "        raise Exception(\"Not allowed more than 2 of deepness\")\n",
    "    \n",
    "    edges = list()\n",
    "    \n",
    "    seed_title = get_wiki_article_by_href(seed_href).title\n",
    "    \n",
    "    done_titles = list()\n",
    "    todo_queue = list()\n",
    "    \n",
    "    todo_queue.append(seed_title)\n",
    "    \n",
    "    for i in range(deepness):\n",
    "        print(\"Working on batch {0} of {1}\".format(i+1, deepness))\n",
    "        \n",
    "        current_todo_queue = todo_queue\n",
    "        todo_queue = list()\n",
    "        \n",
    "        print(\"queue size: {0}\".format(len(current_todo_queue)))\n",
    "        \n",
    "        while len(current_todo_queue) > 0:\n",
    "            \n",
    "            next_title = current_todo_queue.pop() \n",
    "        \n",
    "            if next_title in done_titles:\n",
    "                continue\n",
    "        \n",
    "            edges_scores = get_node_edges_scores(quote(next_title), top)\n",
    "        \n",
    "            done_titles.append(next_title)\n",
    "            \n",
    "            #Sort edges scores and take the top 5\n",
    "            #sorted_edges = sorted(edges_scores.items(), key=lambda a: a[1], reverse=True)[:5]\n",
    "            \n",
    "            #Save new edges and next to do titles\n",
    "            for edge, score in edges_scores.items():\n",
    "                edges.append((edge, score))\n",
    "                \n",
    "                if not edge[1] in done_titles:\n",
    "                    todo_queue.append(edge[1])\n",
    "    \n",
    "    #save_everything() \n",
    "    return edges           \n",
    "\n",
    "    \n",
    "#edges = get_graph_edges(\"JavaScript\", 2, 10)\n",
    "#save_everything()\n",
    "\n",
    "#for e in edges:\n",
    "    #print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_graph(edges):\n",
    "    #Create a directed graph\n",
    "    graph = nx.DiGraph()\n",
    "    for edge in edges:\n",
    "        graph.add_edge(edge[0], edge[1])\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_path_tuples(path):\n",
    "    \"\"\"Function that generates all path tuples from a list path.\"\"\"\n",
    "    path_tuples = []\n",
    "    for i, _ in enumerate(path):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        path_tuples.append((path[i-1], path[i]))\n",
    "\n",
    "    return path_tuples\n",
    "\n",
    "def get_features(graph, seed_node, cutoff, deep_rank_scores=None):\n",
    "    \"\"\"\n",
    "    Function to compute the prereq probabilities for every node.\n",
    "    Compute deeprank and bidirection rank. TO BE EXPLAINED\n",
    "\n",
    "    Returns: bidir_probs, deeprank_probs, n_paths, min_depths, max_depths\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #Compute number of edges per node\n",
    "    nodes_edges = dict()\n",
    "    for n1, n2 in graph.edges():\n",
    "        #init nodes dict if not initiated\n",
    "        if not n1 in nodes_edges:\n",
    "            nodes_edges[n1] = 0\n",
    "        nodes_edges[n1] += 1\n",
    "\n",
    "    #Compute initial edges probabilities\n",
    "    #For bidirection rank we set 1 in case the edge is unidirectional and 0.5 in case bidirectional.\n",
    "    #For deeprank we compute the fraction of the edge over all the edges in the same node.\n",
    "\n",
    "    ### Later we need to create other methods to compute proper distribution for the cases above. ### \n",
    "\n",
    "    bidir_edges_values = dict()\n",
    "    deeprank_edges_values = dict()\n",
    "\n",
    "    #edges_values = dict()\n",
    "    for n1, n2 in graph.edges():\n",
    "        if deep_rank_scores == None:\n",
    "            deeprank_edges_values[(n1, n2)] = 1.0 / nodes_edges[n1]\n",
    "        \n",
    "        if graph.has_edge(n2, n1):\n",
    "            bidir_edges_values[(n1, n2)] = 0.5\n",
    "        else:\n",
    "            bidir_edges_values[(n1, n2)] = 1\n",
    "\n",
    "    if deep_rank_scores != None:\n",
    "        deeprank_edges_values = dict(deep_rank_scores)\n",
    "\n",
    "    #Now compute all the paths to the target seed_node and sequence probabilities to each path\n",
    "    bidir_probs = dict() #Probabilities of reach seed_node from each node based on bidir values\n",
    "    deeprank_probs = dict() #Probabilities of reach seed_node from each node based on deeprank values\n",
    "    min_depths = dict() #Each node min depth\n",
    "    max_depths = dict() #Each node max depth\n",
    "    ns_paths = dict() #Each node number of paths\n",
    "\n",
    "    # create dicts for every feature extracted\n",
    "    # try get insights from kmeans\n",
    "    # try to find something to deploy FAST (the energy applied must be low!)\n",
    "\n",
    "    n_nodes = nx.number_of_nodes(graph)\n",
    "\n",
    "    #Iterate thru all the graph nodes\n",
    "    for i, node in enumerate(graph.nodes()):\n",
    "\n",
    "        print(\"Working on node {0}/{1}\".format(i+1,n_nodes))\n",
    "\n",
    "        #Init min max depth\n",
    "        min_depth = cutoff + 2\n",
    "        max_depth = 0\n",
    "\n",
    "        #Skip seed_node since we do not want verify paths to itself\n",
    "        if node == seed_node:\n",
    "            continue\n",
    "\n",
    "        n_paths = 0\n",
    "        total_bidir_prob = 0\n",
    "        total_deeprank_prob = 0\n",
    "\n",
    "        for path in nx.all_simple_paths(graph, source=seed_node, target=node, cutoff=cutoff):\n",
    "\n",
    "            n_paths += 1\n",
    "            partial_bidir_prob = 1.0\n",
    "            partial_deeprank_prob = 1.0\n",
    "\n",
    "            #Computes min-max depth\n",
    "            max_depth = max(max_depth, len(path))\n",
    "            min_depth = min(min_depth, len(path))\n",
    "\n",
    "            #Iterate the path tuples\n",
    "            for i, edge_tuple in enumerate(extract_path_tuples(path)):\n",
    "                partial_bidir_prob *= bidir_edges_values[edge_tuple]\n",
    "                partial_deeprank_prob *= deeprank_edges_values[edge_tuple]\n",
    "\n",
    "            total_bidir_prob += partial_bidir_prob\n",
    "            total_deeprank_prob += partial_deeprank_prob\n",
    "\n",
    "        #After processing all node paths, save the final values    \n",
    "        bidir_probs[node] = total_bidir_prob / n_paths\n",
    "        deeprank_probs[node] = total_deeprank_prob\n",
    "        min_depths[node] = min_depth\n",
    "        max_depths[node] = max_depth\n",
    "        ns_paths[node] = n_paths \n",
    "\n",
    "    return bidir_probs, deeprank_probs, ns_paths, min_depths, max_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch 1 of 3\n",
      "queue size: 1\n",
      "Tasks to go: 10\n",
      "10%20%30%60%70%50%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Working on batch 2 of 3\n",
      "queue size: 9\n",
      "Tasks to go: 10\n",
      "20%30%60%70%10%80%90%40%50%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "30%40%20%10%70%90%100%80%60%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 6\n",
      ".50%..100%.\n",
      "Finishing downloading. Done tasks: 6/6\n",
      "Tasks to go: 10\n",
      "10%20%30%50%40%60%80%90%70%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%30%40%80%50%90%70%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%40%50%60%30%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%30%100%40%50%60%90%70%80%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "20%30%40%10%70%60%50%100%90%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%40%30%60%80%90%70%100%50%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Working on batch 3 of 3\n",
      "queue size: 79\n",
      "Tasks to go: 10\n",
      "10%20%40%30%60%80%70%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "20%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%30%40%50%60%70%80%90%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "20%10%30%40%70%80%50%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "20%10%30%60%80%50%70%90%40%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "20%10%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "20%70%30%10%50%60%40%80%90%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "20%10%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "40%10%30%60%20%50%70%80%90%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "20%10%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%30%40%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%40%30%20%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "20%10%40%30%50%60%70%80%90%100%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "10%20%40%30%50%\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 6\n",
      "\n",
      "Finishing downloading. Done tasks: 6/6\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Tasks to go: 10\n",
      "\n",
      "Finishing downloading. Done tasks: 10/10\n",
      "Working on node 1/505\n",
      "Working on node 2/505\n",
      "Working on node 3/505\n",
      "Working on node 4/505\n",
      "Working on node 5/505\n",
      "Working on node 6/505\n",
      "Working on node 7/505\n",
      "Working on node 8/505\n",
      "Working on node 9/505\n",
      "Working on node 10/505\n",
      "Working on node 11/505\n",
      "Working on node 12/505\n",
      "Working on node 13/505\n",
      "Working on node 14/505\n",
      "Working on node 15/505\n",
      "Working on node 16/505\n",
      "Working on node 17/505\n",
      "Working on node 18/505\n",
      "Working on node 19/505\n",
      "Working on node 20/505\n",
      "Working on node 21/505\n",
      "Working on node 22/505\n",
      "Working on node 23/505\n",
      "Working on node 24/505\n",
      "Working on node 25/505\n",
      "Working on node 26/505\n",
      "Working on node 27/505\n",
      "Working on node 28/505\n",
      "Working on node 29/505\n",
      "Working on node 30/505\n",
      "Working on node 31/505\n",
      "Working on node 32/505\n",
      "Working on node 33/505\n",
      "Working on node 34/505\n",
      "Working on node 35/505\n",
      "Working on node 36/505\n",
      "Working on node 37/505\n",
      "Working on node 38/505\n",
      "Working on node 39/505\n",
      "Working on node 40/505\n",
      "Working on node 41/505\n",
      "Working on node 42/505\n",
      "Working on node 43/505\n",
      "Working on node 44/505\n",
      "Working on node 45/505\n",
      "Working on node 46/505\n",
      "Working on node 47/505\n",
      "Working on node 48/505\n",
      "Working on node 49/505\n",
      "Working on node 50/505\n",
      "Working on node 51/505\n",
      "Working on node 52/505\n",
      "Working on node 53/505\n",
      "Working on node 54/505\n",
      "Working on node 55/505\n",
      "Working on node 56/505\n",
      "Working on node 57/505\n",
      "Working on node 58/505\n",
      "Working on node 59/505\n",
      "Working on node 60/505\n",
      "Working on node 61/505\n",
      "Working on node 62/505\n",
      "Working on node 63/505\n",
      "Working on node 64/505\n",
      "Working on node 65/505\n",
      "Working on node 66/505\n",
      "Working on node 67/505\n",
      "Working on node 68/505\n",
      "Working on node 69/505\n",
      "Working on node 70/505\n",
      "Working on node 71/505\n",
      "Working on node 72/505\n",
      "Working on node 73/505\n",
      "Working on node 74/505\n",
      "Working on node 75/505\n",
      "Working on node 76/505\n",
      "Working on node 77/505\n",
      "Working on node 78/505\n",
      "Working on node 79/505\n",
      "Working on node 80/505\n",
      "Working on node 81/505\n",
      "Working on node 82/505\n",
      "Working on node 83/505\n",
      "Working on node 84/505\n",
      "Working on node 85/505\n",
      "Working on node 86/505\n",
      "Working on node 87/505\n",
      "Working on node 88/505\n",
      "Working on node 89/505\n",
      "Working on node 90/505\n",
      "Working on node 91/505\n",
      "Working on node 92/505\n",
      "Working on node 93/505\n",
      "Working on node 94/505\n",
      "Working on node 95/505\n",
      "Working on node 96/505\n",
      "Working on node 97/505\n",
      "Working on node 98/505\n",
      "Working on node 99/505\n",
      "Working on node 100/505\n",
      "Working on node 101/505\n",
      "Working on node 102/505\n",
      "Working on node 103/505\n",
      "Working on node 104/505\n",
      "Working on node 105/505\n",
      "Working on node 106/505\n",
      "Working on node 107/505\n",
      "Working on node 108/505\n",
      "Working on node 109/505\n",
      "Working on node 110/505\n",
      "Working on node 111/505\n",
      "Working on node 112/505\n",
      "Working on node 113/505\n",
      "Working on node 114/505\n",
      "Working on node 115/505\n",
      "Working on node 116/505\n",
      "Working on node 117/505\n",
      "Working on node 118/505\n",
      "Working on node 119/505\n",
      "Working on node 120/505\n",
      "Working on node 121/505\n",
      "Working on node 122/505\n",
      "Working on node 123/505\n",
      "Working on node 124/505\n",
      "Working on node 125/505\n",
      "Working on node 126/505\n",
      "Working on node 127/505\n",
      "Working on node 128/505\n",
      "Working on node 129/505\n",
      "Working on node 130/505\n",
      "Working on node 131/505\n",
      "Working on node 132/505\n",
      "Working on node 133/505\n",
      "Working on node 134/505\n",
      "Working on node 135/505\n",
      "Working on node 136/505\n",
      "Working on node 137/505\n",
      "Working on node 138/505\n",
      "Working on node 139/505\n",
      "Working on node 140/505\n",
      "Working on node 141/505\n",
      "Working on node 142/505\n",
      "Working on node 143/505\n",
      "Working on node 144/505\n",
      "Working on node 145/505\n",
      "Working on node 146/505\n",
      "Working on node 147/505\n",
      "Working on node 148/505\n",
      "Working on node 149/505\n",
      "Working on node 150/505\n",
      "Working on node 151/505\n",
      "Working on node 152/505\n",
      "Working on node 153/505\n",
      "Working on node 154/505\n",
      "Working on node 155/505\n",
      "Working on node 156/505\n",
      "Working on node 157/505\n",
      "Working on node 158/505\n",
      "Working on node 159/505\n",
      "Working on node 160/505\n",
      "Working on node 161/505\n",
      "Working on node 162/505\n",
      "Working on node 163/505\n",
      "Working on node 164/505\n",
      "Working on node 165/505\n",
      "Working on node 166/505\n",
      "Working on node 167/505\n",
      "Working on node 168/505\n",
      "Working on node 169/505\n",
      "Working on node 170/505\n",
      "Working on node 171/505\n",
      "Working on node 172/505\n",
      "Working on node 173/505\n",
      "Working on node 174/505\n",
      "Working on node 175/505\n",
      "Working on node 176/505\n",
      "Working on node 177/505\n",
      "Working on node 178/505\n",
      "Working on node 179/505\n",
      "Working on node 180/505\n",
      "Working on node 181/505\n",
      "Working on node 182/505\n",
      "Working on node 183/505\n",
      "Working on node 184/505\n",
      "Working on node 185/505\n",
      "Working on node 186/505\n",
      "Working on node 187/505\n",
      "Working on node 188/505\n",
      "Working on node 189/505\n",
      "Working on node 190/505\n",
      "Working on node 191/505\n",
      "Working on node 192/505\n",
      "Working on node 193/505\n",
      "Working on node 194/505\n",
      "Working on node 195/505\n",
      "Working on node 196/505\n",
      "Working on node 197/505\n",
      "Working on node 198/505\n",
      "Working on node 199/505\n",
      "Working on node 200/505\n",
      "Working on node 201/505\n",
      "Working on node 202/505\n",
      "Working on node 203/505\n",
      "Working on node 204/505\n",
      "Working on node 205/505\n",
      "Working on node 206/505\n",
      "Working on node 207/505\n",
      "Working on node 208/505\n",
      "Working on node 209/505\n",
      "Working on node 210/505\n",
      "Working on node 211/505\n",
      "Working on node 212/505\n",
      "Working on node 213/505\n",
      "Working on node 214/505\n",
      "Working on node 215/505\n",
      "Working on node 216/505\n",
      "Working on node 217/505\n",
      "Working on node 218/505\n",
      "Working on node 219/505\n",
      "Working on node 220/505\n",
      "Working on node 221/505\n",
      "Working on node 222/505\n",
      "Working on node 223/505\n",
      "Working on node 224/505\n",
      "Working on node 225/505\n",
      "Working on node 226/505\n",
      "Working on node 227/505\n",
      "Working on node 228/505\n",
      "Working on node 229/505\n",
      "Working on node 230/505\n",
      "Working on node 231/505\n",
      "Working on node 232/505\n",
      "Working on node 233/505\n",
      "Working on node 234/505\n",
      "Working on node 235/505\n",
      "Working on node 236/505\n",
      "Working on node 237/505\n",
      "Working on node 238/505\n",
      "Working on node 239/505\n",
      "Working on node 240/505\n",
      "Working on node 241/505\n",
      "Working on node 242/505\n",
      "Working on node 243/505\n",
      "Working on node 244/505\n",
      "Working on node 245/505\n",
      "Working on node 246/505\n",
      "Working on node 247/505\n",
      "Working on node 248/505\n",
      "Working on node 249/505\n",
      "Working on node 250/505\n",
      "Working on node 251/505\n",
      "Working on node 252/505\n",
      "Working on node 253/505\n",
      "Working on node 254/505\n",
      "Working on node 255/505\n",
      "Working on node 256/505\n",
      "Working on node 257/505\n",
      "Working on node 258/505\n",
      "Working on node 259/505\n",
      "Working on node 260/505\n",
      "Working on node 261/505\n",
      "Working on node 262/505\n",
      "Working on node 263/505\n",
      "Working on node 264/505\n",
      "Working on node 265/505\n",
      "Working on node 266/505\n",
      "Working on node 267/505\n",
      "Working on node 268/505\n",
      "Working on node 269/505\n",
      "Working on node 270/505\n",
      "Working on node 271/505\n",
      "Working on node 272/505\n",
      "Working on node 273/505\n",
      "Working on node 274/505\n",
      "Working on node 275/505\n",
      "Working on node 276/505\n",
      "Working on node 277/505\n",
      "Working on node 278/505\n",
      "Working on node 279/505\n",
      "Working on node 280/505\n",
      "Working on node 281/505\n",
      "Working on node 282/505\n",
      "Working on node 283/505\n",
      "Working on node 284/505\n",
      "Working on node 285/505\n",
      "Working on node 286/505\n",
      "Working on node 287/505\n",
      "Working on node 288/505\n",
      "Working on node 289/505\n",
      "Working on node 290/505\n",
      "Working on node 291/505\n",
      "Working on node 292/505\n",
      "Working on node 293/505\n",
      "Working on node 294/505\n",
      "Working on node 295/505\n",
      "Working on node 296/505\n",
      "Working on node 297/505\n",
      "Working on node 298/505\n",
      "Working on node 299/505\n",
      "Working on node 300/505\n",
      "Working on node 301/505\n",
      "Working on node 302/505\n",
      "Working on node 303/505\n",
      "Working on node 304/505\n",
      "Working on node 305/505\n",
      "Working on node 306/505\n",
      "Working on node 307/505\n",
      "Working on node 308/505\n",
      "Working on node 309/505\n",
      "Working on node 310/505\n",
      "Working on node 311/505\n",
      "Working on node 312/505\n",
      "Working on node 313/505\n",
      "Working on node 314/505\n",
      "Working on node 315/505\n",
      "Working on node 316/505\n",
      "Working on node 317/505\n",
      "Working on node 318/505\n",
      "Working on node 319/505\n",
      "Working on node 320/505\n",
      "Working on node 321/505\n",
      "Working on node 322/505\n",
      "Working on node 323/505\n",
      "Working on node 324/505\n",
      "Working on node 325/505\n",
      "Working on node 326/505\n",
      "Working on node 327/505\n",
      "Working on node 328/505\n",
      "Working on node 329/505\n",
      "Working on node 330/505\n",
      "Working on node 331/505\n",
      "Working on node 332/505\n",
      "Working on node 333/505\n",
      "Working on node 334/505\n",
      "Working on node 335/505\n",
      "Working on node 336/505\n",
      "Working on node 337/505\n",
      "Working on node 338/505\n",
      "Working on node 339/505\n",
      "Working on node 340/505\n",
      "Working on node 341/505\n",
      "Working on node 342/505\n",
      "Working on node 343/505\n",
      "Working on node 344/505\n",
      "Working on node 345/505\n",
      "Working on node 346/505\n",
      "Working on node 347/505\n",
      "Working on node 348/505\n",
      "Working on node 349/505\n",
      "Working on node 350/505\n",
      "Working on node 351/505\n",
      "Working on node 352/505\n",
      "Working on node 353/505\n",
      "Working on node 354/505\n",
      "Working on node 355/505\n",
      "Working on node 356/505\n",
      "Working on node 357/505\n",
      "Working on node 358/505\n",
      "Working on node 359/505\n",
      "Working on node 360/505\n",
      "Working on node 361/505\n",
      "Working on node 362/505\n",
      "Working on node 363/505\n",
      "Working on node 364/505\n",
      "Working on node 365/505\n",
      "Working on node 366/505\n",
      "Working on node 367/505\n",
      "Working on node 368/505\n",
      "Working on node 369/505\n",
      "Working on node 370/505\n",
      "Working on node 371/505\n",
      "Working on node 372/505\n",
      "Working on node 373/505\n",
      "Working on node 374/505\n",
      "Working on node 375/505\n",
      "Working on node 376/505\n",
      "Working on node 377/505\n",
      "Working on node 378/505\n",
      "Working on node 379/505\n",
      "Working on node 380/505\n",
      "Working on node 381/505\n",
      "Working on node 382/505\n",
      "Working on node 383/505\n",
      "Working on node 384/505\n",
      "Working on node 385/505\n",
      "Working on node 386/505\n",
      "Working on node 387/505\n",
      "Working on node 388/505\n",
      "Working on node 389/505\n",
      "Working on node 390/505\n",
      "Working on node 391/505\n",
      "Working on node 392/505\n",
      "Working on node 393/505\n",
      "Working on node 394/505\n",
      "Working on node 395/505\n",
      "Working on node 396/505\n",
      "Working on node 397/505\n",
      "Working on node 398/505\n",
      "Working on node 399/505\n",
      "Working on node 400/505\n",
      "Working on node 401/505\n",
      "Working on node 402/505\n",
      "Working on node 403/505\n",
      "Working on node 404/505\n",
      "Working on node 405/505\n",
      "Working on node 406/505\n",
      "Working on node 407/505\n",
      "Working on node 408/505\n",
      "Working on node 409/505\n",
      "Working on node 410/505\n",
      "Working on node 411/505\n",
      "Working on node 412/505\n",
      "Working on node 413/505\n",
      "Working on node 414/505\n",
      "Working on node 415/505\n",
      "Working on node 416/505\n",
      "Working on node 417/505\n",
      "Working on node 418/505\n",
      "Working on node 419/505\n",
      "Working on node 420/505\n",
      "Working on node 421/505\n",
      "Working on node 422/505\n",
      "Working on node 423/505\n",
      "Working on node 424/505\n",
      "Working on node 425/505\n",
      "Working on node 426/505\n",
      "Working on node 427/505\n",
      "Working on node 428/505\n",
      "Working on node 429/505\n",
      "Working on node 430/505\n",
      "Working on node 431/505\n",
      "Working on node 432/505\n",
      "Working on node 433/505\n",
      "Working on node 434/505\n",
      "Working on node 435/505\n",
      "Working on node 436/505\n",
      "Working on node 437/505\n",
      "Working on node 438/505\n",
      "Working on node 439/505\n",
      "Working on node 440/505\n",
      "Working on node 441/505\n",
      "Working on node 442/505\n",
      "Working on node 443/505\n",
      "Working on node 444/505\n",
      "Working on node 445/505\n",
      "Working on node 446/505\n",
      "Working on node 447/505\n",
      "Working on node 448/505\n",
      "Working on node 449/505\n",
      "Working on node 450/505\n",
      "Working on node 451/505\n",
      "Working on node 452/505\n",
      "Working on node 453/505\n",
      "Working on node 454/505\n",
      "Working on node 455/505\n",
      "Working on node 456/505\n",
      "Working on node 457/505\n",
      "Working on node 458/505\n",
      "Working on node 459/505\n",
      "Working on node 460/505\n",
      "Working on node 461/505\n",
      "Working on node 462/505\n",
      "Working on node 463/505\n",
      "Working on node 464/505\n",
      "Working on node 465/505\n",
      "Working on node 466/505\n",
      "Working on node 467/505\n",
      "Working on node 468/505\n",
      "Working on node 469/505\n",
      "Working on node 470/505\n",
      "Working on node 471/505\n",
      "Working on node 472/505\n",
      "Working on node 473/505\n",
      "Working on node 474/505\n",
      "Working on node 475/505\n",
      "Working on node 476/505\n",
      "Working on node 477/505\n",
      "Working on node 478/505\n",
      "Working on node 479/505\n",
      "Working on node 480/505\n",
      "Working on node 481/505\n",
      "Working on node 482/505\n",
      "Working on node 483/505\n",
      "Working on node 484/505\n",
      "Working on node 485/505\n",
      "Working on node 486/505\n",
      "Working on node 487/505\n",
      "Working on node 488/505\n",
      "Working on node 489/505\n",
      "Working on node 490/505\n",
      "Working on node 491/505\n",
      "Working on node 492/505\n",
      "Working on node 493/505\n",
      "Working on node 494/505\n",
      "Working on node 495/505\n",
      "Working on node 496/505\n",
      "Working on node 497/505\n",
      "Working on node 498/505\n",
      "Working on node 499/505\n",
      "Working on node 500/505\n",
      "Working on node 501/505\n",
      "Working on node 502/505\n",
      "Working on node 503/505\n",
      "Working on node 504/505\n",
      "Working on node 505/505\n",
      "\n",
      "\n",
      "International Standard Book Number        115        0.802        115        3        5\n",
      "Digital object identifier        73        0.716        73        2        5\n",
      "International Organization for Standardization        70        0.864        70        3        5\n",
      "Library        39        0.91        39        4        5\n",
      "Metadata        36        0.819        36        3        5\n",
      "10        32        0.906        32        4        5\n",
      "11 (number)        32        0.906        32        4        5\n",
      "Check digit        32        0.906        32        4        5\n",
      "Book        32        0.906        32        4        5\n",
      "Portable Document Format        32        0.906        32        4        5\n",
      "Identifier        32        0.906        32        4        5\n",
      "International Article Number        32        0.906        32        4        5\n",
      "Handle (computing)        31        0.79        31        3        5\n",
      "URL        31        0.79        31        3        5\n",
      "Data        30        0.767        30        4        5\n",
      "Crossref        28        0.42        28        3        5\n",
      "PubMed        28        0.67        28        3        5\n",
      "Handle System        26        0.423        26        3        5\n",
      "Indecs Content Model        26        0.846        26        3        5\n",
      "Mathematics        23        0.848        23        3        5\n",
      "Machine learning        21        0.583        21        3        5\n",
      "Statistics        19        0.658        19        3        5\n",
      "Brain        18        0.611        18        3        5\n",
      "Logic        18        0.889        18        4        5\n",
      "Set (mathematics)        17        0.882        17        4        5\n",
      "Algorithm        16        0.688        16        3        5\n",
      "Function (mathematics)        16        0.688        16        3        5\n",
      "Science (journal)        15        0.833        15        3        5\n",
      "Statistical classification        15        0.633        15        4        5\n",
      "Axon        15        0.667        15        4        5\n",
      "Standardization        15        0.833        15        4        5\n",
      "International Electrotechnical Commission        15        0.833        15        4        5\n",
      "Neuron        14        0.554        14        3        5\n",
      "Neuroscience        14        0.625        14        4        5\n",
      "PubMed Central        12        0.833        12        4        5\n",
      "Integer        12        0.875        12        4        5\n",
      "Content (media)        12        0.958        12        4        5\n",
      "French language        12        0.958        12        4        5\n",
      "Artificial intelligence        11        0.682        11        3        5\n",
      "Science        11        0.864        11        4        5\n",
      "Domain of a function        11        0.864        11        4        5\n",
      "Codomain        11        0.864        11        4        5\n",
      "ISO image        11        0.955        11        4        5\n",
      "International standard        11        0.955        11        4        5\n",
      "ISO/IEC JTC 1        11        0.955        11        4        5\n",
      "Working group        11        0.955        11        4        5\n",
      "Russian language        11        0.955        11        4        5\n",
      "Office Open XML        11        0.955        11        4        5\n",
      "Hypertext Transfer Protocol        11        0.727        11        4        5\n",
      "Request for Comments        11        0.727        11        4        5\n",
      "Knowledge        10        0.8        10        4        5\n",
      "Machine Learning (journal)        10        0.7        10        4        5\n",
      "Unsupervised learning        10        0.7        10        4        5\n",
      "Data mining        10        0.7        10        4        5\n",
      "MEDLINE        10        0.85        10        4        5\n",
      "Medical Subject Headings        10        0.85        10        4        5\n",
      "United States National Library of Medicine        10        0.85        10        4        5\n",
      "The BMJ        10        0.85        10        4        5\n",
      "Anne O'Tate        10        0.85        10        4        5\n",
      "GoPubMed        10        0.85        10        4        5\n",
      "Entrez        10        0.85        10        4        5\n",
      "Value (mathematics)        10        0.85        10        4        5\n",
      "Image (mathematics)        10        0.85        10        4        5\n",
      "Subset        10        0.85        10        4        5\n",
      "Variable (mathematics)        10        0.85        10        4        5\n",
      "Learning        9        0.833        9        3        5\n",
      "Memory        9        0.722        9        4        5\n",
      "Visual perception        9        0.667        9        4        5\n",
      "Artificial neural network        8        0.812        8        2        5\n",
      "Synapse        8        0.531        8        4        5\n",
      "Dendrite        8        0.625        8        4        5\n",
      "Computer memory        8        0.875        8        4        5\n",
      "Reference (computer science)        8        0.875        8        4        5\n",
      "Null hypothesis        8        0.875        8        4        5\n",
      "Computational statistics        8        0.875        8        4        5\n",
      "Statistic        8        0.875        8        4        5\n",
      "Statistical population        8        0.875        8        4        5\n",
      "Sample (statistics)        8        0.875        8        4        5\n",
      "Design of experiments        8        0.875        8        4        5\n",
      "Cell (biology)        7        0.643        7        3        5\n",
      "Pointer (computer programming)        7        0.857        7        3        5\n",
      "Research        7        0.786        7        3        5\n",
      "Academic journal        7        0.679        7        4        5\n",
      "Journal Citation Reports        7        0.679        7        4        5\n",
      "Nature (journal)        7        0.679        7        4        5\n",
      "International Standard Serial Number        7        0.571        7        4        5\n",
      "Operating system        7        0.857        7        4        5\n",
      "Database        7        0.929        7        4        5\n",
      "Technical standard        7        0.929        7        4        5\n",
      "Museum        7        0.929        7        4        5\n",
      "Dublin Core        7        0.929        7        4        5\n",
      "Computer file        7        0.929        7        4        5\n",
      "Convex function        7        0.714        7        4        5\n",
      "Retina        6        0.667        6        3        5\n",
      "Artificial neuron        6        0.417        6        3        5\n",
      "C++        6        0.583        6        3        5\n",
      "Human brain        6        0.75        6        4        5\n",
      "Nervous system        6        0.75        6        4        5\n",
      "Vertebrate        6        0.75        6        4        5\n",
      "Human        6        0.75        6        4        5\n",
      "Glutamic acid        6        0.583        6        4        5\n",
      "Number        6        0.917        6        4        5\n",
      "Mathematician        6        0.917        6        4        5\n",
      "Geometry        6        0.917        6        4        5\n",
      "Mathematical analysis        6        0.917        6        4        5\n",
      "Applied mathematics        6        0.917        6        4        5\n",
      "Information        6        0.833        6        4        5\n",
      "Euclid        6        0.917        6        4        5\n",
      "Euclidean algorithm        6        0.917        6        4        5\n",
      "P (complexity)        6        0.917        6        4        5\n",
      "Stephen Cole Kleene        6        0.917        6        4        5\n",
      "Computation        6        0.917        6        4        5\n",
      "0        6        0.917        6        4        5\n",
      "Digital content        6        0.5        6        4        5\n",
      "Publishing        6        0.5        6        4        5\n",
      "Initiative for Open Citations        6        0.5        6        4        5\n",
      "Nonprofit organization        6        0.5        6        4        5\n",
      "Plagiarism        6        0.5        6        4        5\n",
      "Proceedings        6        0.5        6        4        5\n",
      "Association of Learned and Professional Society Publishers        6        0.5        6        4        5\n",
      "Technical report        6        0.5        6        4        5\n",
      "Uniform Resource Identifier        6        0.917        6        4        5\n",
      "World Wide Web        6        0.917        6        4        5\n",
      "HTML        6        0.917        6        4        5\n",
      "Internet Engineering Task Force        6        0.917        6        4        5\n",
      "World Wide Web Consortium        6        0.917        6        4        5\n",
      "Domain name        6        0.917        6        4        5\n",
      "Process (computing)        6        0.917        6        4        5\n",
      "System resource        6        0.917        6        4        5\n",
      "File descriptor        6        0.917        6        4        5\n",
      "Software        6        0.917        6        4        5\n",
      "Opaque data type        6        0.917        6        4        5\n",
      "Gradient        6        0.75        6        4        5\n",
      "Overfitting        5        0.475        5        2        5\n",
      "Gradient descent        5        0.5        5        3        5\n",
      "Test set        5        0.275        5        3        5\n",
      "Statistical model        5        0.7        5        3        5\n",
      "Neural Networks (journal)        5        0.4        5        3        5\n",
      "Biological neural network        5        0.4        5        3        5\n",
      "Deep learning        5        0.4        5        3        5\n",
      "Scholarly method        5        0.65        5        4        5\n",
      "American Association for the Advancement of Science        5        0.8        5        4        5\n",
      "Bruce Alberts        5        0.8        5        4        5\n",
      "Marcia McNutt        5        0.8        5        4        5\n",
      "Science Advances        5        0.8        5        4        5\n",
      "Human genome        5        0.8        5        4        5\n",
      "Daniel E. Koshland Jr.        5        0.8        5        4        5\n",
      "Leland Ossian Howard        5        0.8        5        4        5\n",
      "Scientific modelling        5        0.8        5        4        5\n",
      "Statistical inference        5        0.8        5        4        5\n",
      "Regression analysis        5        0.8        5        4        5\n",
      "Mathematical model        5        0.8        5        4        5\n",
      "Interoperability        5        1.0        5        4        5\n",
      "Commerce        5        1.0        5        4        5\n",
      "Product life-cycle management (marketing)        5        1.0        5        4        5\n",
      "E-commerce        5        1.0        5        4        5\n",
      "Semantic interoperability        5        1.0        5        4        5\n",
      "WIPO Copyright Treaty        5        1.0        5        4        5\n",
      "Functional Requirements for Bibliographic Records        5        1.0        5        4        5\n",
      "Interdisciplinary Description of Complex Systems        5        1.0        5        4        5\n",
      "Persistent identifier        5        0.5        5        4        5\n",
      "International Telecommunication Union        5        0.5        5        4        5\n",
      "Proxy server        5        0.5        5        4        5\n",
      "Mathematical optimization        5        0.7        5        4        5\n",
      "ArXiv        4        0.562        4        2        5\n",
      "Light        4        0.75        4        3        5\n",
      "Retinal ganglion cell        4        0.562        4        3        5\n",
      "Least squares        4        0.75        4        3        5\n",
      "Tikhonov regularization        4        0.375        4        3        5\n",
      "Early stopping        4        0.281        4        3        5\n",
      "Soma (biology)        4        0.562        4        4        5\n",
      "Peer review        4        0.562        4        4        5\n",
      "Open access        4        0.562        4        4        5\n",
      "Institutional repository        4        0.562        4        4        5\n",
      "Statistical parameter        4        0.75        4        4        5\n",
      "Mean        4        0.75        4        4        5\n",
      "Class (computer programming)        4        0.5        4        4        5\n",
      "Software feature        4        0.5        4        4        5\n",
      "Template (C++)        4        0.5        4        4        5\n",
      "Array data structure        4        0.625        4        4        5\n",
      "Intelligence        4        0.875        4        4        5\n",
      "Machine        4        0.875        4        4        5\n",
      "Mind (journal)        4        0.875        4        4        5\n",
      "Mind        4        0.875        4        4        5\n",
      "Retinal        4        0.75        4        4        5\n",
      "Gene therapy        4        0.75        4        4        5\n",
      "Cone cell        4        0.75        4        4        5\n",
      "Rod cell        4        0.75        4        4        5\n",
      "Regularization (mathematics)        3        0.542        3        2        5\n",
      "Visual system        3        0.667        3        3        4\n",
      "Array data type        3        0.667        3        3        5\n",
      "Parameter        3        0.667        3        3        5\n",
      "Linear regression        3        0.667        3        3        5\n",
      "Predictive inference        3        0.667        3        3        5\n",
      "Occam's razor        3        0.667        3        3        5\n",
      "Model selection        3        0.667        3        3        5\n",
      "VC dimension        3        0.667        3        3        5\n",
      "Group (mathematics)        3        0.833        3        3        5\n",
      "Physics        3        0.667        3        3        5\n",
      "Disciplinary repository        3        0.333        3        3        5\n",
      "Archive        3        0.667        3        3        5\n",
      "Eprint        3        0.667        3        3        5\n",
      "Preprint        3        0.333        3        3        5\n",
      "Art        3        0.833        3        4        5\n",
      "Academic authorship        3        0.833        3        4        5\n",
      "Hypothesis        3        0.833        3        4        5\n",
      "Journal        3        0.833        3        4        5\n",
      "History        3        0.833        3        4        5\n",
      "Play (activity)        3        0.667        3        4        5\n",
      "Education        3        0.667        3        4        5\n",
      "Behavior        3        0.667        3        4        5\n",
      "Stimulation        3        0.667        3        4        5\n",
      "Attention        3        0.667        3        4        5\n",
      "Habituation        3        0.667        3        4        5\n",
      "Greek language        3        0.833        3        4        5\n",
      "Addition        3        1.0        3        4        5\n",
      "Operator (mathematics)        3        0.833        3        4        5\n",
      "Voltage        3        1.0        3        4        5\n",
      "Frequency domain        3        0.833        3        4        5\n",
      "Transfer function        3        0.667        3        4        5\n",
      "Imaginary unit        3        0.667        3        4        5\n",
      "Probability distribution        3        0.833        3        4        5\n",
      "Dimension        3        0.833        3        4        5\n",
      "Normal distribution        3        0.833        3        4        5\n",
      "Stochastic        3        0.833        3        4        5\n",
      "Cambridge University Press        3        0.833        3        4        5\n",
      "Independent and identically distributed random variables        3        0.833        3        4        5\n",
      "Independence (probability theory)        3        0.333        3        4        5\n",
      "Supervised learning        3        0.333        3        4        5\n",
      "Syntax (programming languages)        3        0.5        3        4        5\n",
      "Variable (computer science)        3        0.5        3        4        5\n",
      "Maxima and minima        3        0.667        3        4        5\n",
      "Line search        3        0.667        3        4        5\n",
      "Preconditioner        3        0.667        3        4        5\n",
      "Conjugate gradient method        3        0.667        3        4        5\n",
      "Method of steepest descent        3        0.667        3        4        5\n",
      "Generalization error        3        0.333        3        4        5\n",
      "Boosting (machine learning)        3        0.333        3        4        5\n",
      "Nonparametric regression        3        0.333        3        4        5\n",
      "Ordinary least squares        3        0.417        3        4        5\n",
      "Well-posed problem        3        0.417        3        4        5\n",
      "Singular value decomposition        3        0.417        3        4        5\n",
      "Residual sum of squares        3        0.417        3        4        5\n",
      "Covariance matrix        3        0.417        3        4        5\n",
      "Wiener filter        3        0.417        3        4        5\n",
      "Integral equation        3        0.417        3        4        5\n",
      "Linearity        3        0.833        3        4        5\n",
      "Taxicab geometry        3        0.833        3        4        5\n",
      "Linear least squares (mathematics)        3        0.833        3        4        5\n",
      "Dependent and independent variables        3        0.833        3        4        5\n",
      "Errors and residuals        3        0.833        3        4        5\n",
      "Carl Friedrich Gauss        3        0.833        3        4        5\n",
      "Uncorrelated random variables        3        0.833        3        4        5\n",
      "Eye        3        0.667        3        4        5\n",
      "Lateral intraparietal cortex        3        0.667        3        4        5\n",
      "C (programming language)        2        0.75        2        2        5\n",
      "Norm (mathematics)        2        0.75        2        3        4\n",
      "Proximal operator        2        0.75        2        3        4\n",
      "Elastic net regularization        2        0.375        2        3        4\n",
      "Programming language        2        0.5        2        3        4\n",
      "Fourier transform        2        0.75        2        3        4\n",
      "Signal processing        2        1.0        2        3        4\n",
      "Speech recognition        2        0.375        2        4        5\n",
      "Training        2        0.375        2        4        5\n",
      "Feature (machine learning)        2        0.375        2        4        5\n",
      "Neuroimaging        2        0.375        2        4        5\n",
      "Action potential        2        0.375        2        4        5\n",
      "Functional neuroimaging        2        0.375        2        4        5\n",
      "Excitatory postsynaptic potential        2        0.375        2        4        5\n",
      "Psychology        2        0.375        2        4        5\n",
      "Elsevier        2        0.375        2        4        5\n",
      "Scientific journal        2        0.375        2        4        5\n",
      "Okinawa Institute of Science and Technology        2        0.375        2        4        5\n",
      "Boston University        2        0.375        2        4        5\n",
      "Science Citation Index        2        0.375        2        4        5\n",
      "European Neural Network Society        2        0.375        2        4        5\n",
      "Stephen Grossberg        2        0.375        2        4        5\n",
      "BioRxiv        2        0.375        2        4        5\n",
      "SciELO        2        0.375        2        4        5\n",
      "Center for Open Science        2        0.375        2        4        5\n",
      "Digital library        2        0.75        2        4        5\n",
      "Stevan Harnad        2        0.75        2        4        5\n",
      "Thesis        2        0.75        2        4        5\n",
      "Electronic journal        2        0.75        2        4        5\n",
      "Academic publishing        2        0.75        2        4        5\n",
      "Electronic publishing        2        0.75        2        4        5\n",
      "Electronic article        2        0.75        2        4        5\n",
      "National archives        2        0.75        2        4        5\n",
      "National Archives of India        2        0.75        2        4        5\n",
      "Archives Nationales (France)        2        0.75        2        4        5\n",
      "Archivist        2        0.75        2        4        5\n",
      "United States        2        0.75        2        4        5\n",
      "Society of American Archivists        2        0.75        2        4        5\n",
      "Archival science        2        0.75        2        4        5\n",
      "Records management        2        0.75        2        4        5\n",
      "List of historians        2        0.75        2        4        5\n",
      "Social Science Research Network        2        0.375        2        4        5\n",
      "Econstor        2        0.375        2        4        5\n",
      "List of repositories        2        0.375        2        4        5\n",
      "CiteSeerX        2        0.375        2        4        5\n",
      "Research Papers in Economics        2        0.375        2        4        5\n",
      "Astrophysics Data System        2        0.375        2        4        5\n",
      "Physics (Aristotle)        2        0.75        2        4        5\n",
      "Matter        2        0.75        2        4        5\n",
      "Mechanics        2        0.75        2        4        5\n",
      "Quantum        2        0.75        2        4        5\n",
      "Theory        2        0.75        2        4        5\n",
      "Energy        2        0.75        2        4        5\n",
      "Astronomy        2        0.75        2        4        5\n",
      "Signalling theory        2        1.0        2        4        5\n",
      "Digital signal processing        2        1.0        2        4        5\n",
      "Filter (signal processing)        2        1.0        2        4        5\n",
      "Image processing        2        1.0        2        4        5\n",
      "Discrete-time signal        2        1.0        2        4        5\n",
      "Equalization (communications)        2        1.0        2        4        5\n",
      "Laplace transform        2        0.5        2        4        4\n",
      "Group theory        2        1.0        2        4        5\n",
      "Symmetry        2        1.0        2        4        5\n",
      "Element (mathematics)        2        1.0        2        4        5\n",
      "Subgroup        2        1.0        2        4        5\n",
      "Order (group theory)        2        1.0        2        4        5\n",
      "Xi (letter)        2        0.75        2        4        5\n",
      "Joseph Fourier        2        0.75        2        4        5\n",
      "Omega        2        0.75        2        4        5\n",
      "Transformation (function)        2        0.75        2        4        5\n",
      "Circumflex        2        0.75        2        4        5\n",
      "Frequency        2        0.75        2        4        5\n",
      "Shattered set        2        0.75        2        4        5\n",
      "Polynomial        2        0.75        2        4        5\n",
      "Cardinality        2        0.75        2        4        5\n",
      "Upper and lower bounds        2        0.75        2        4        5\n",
      "Ε-net (computational geometry)        2        0.75        2        4        5\n",
      "All models are wrong        2        0.75        2        4        5\n",
      "Ecology (journal)        2        0.75        2        4        5\n",
      "Goodness of fit        2        0.75        2        4        5\n",
      "Bayes factor        2        0.75        2        4        5\n",
      "Algorithmic information theory        2        0.75        2        4        5\n",
      "Razor (philosophy)        2        0.75        2        4        5\n",
      "Simplicity        2        0.75        2        4        5\n",
      "William of Ockham        2        0.75        2        4        5\n",
      "Punishment        2        0.75        2        4        5\n",
      "Philosophy of science        2        0.75        2        4        5\n",
      "Information theory        2        0.75        2        4        5\n",
      "Seymour Geisser        2        0.75        2        4        5\n",
      "Bruno de Finetti        2        0.75        2        4        5\n",
      "Exchangeable random variables        2        0.75        2        4        5\n",
      "Celestial mechanics        2        0.75        2        4        5\n",
      "Prediction interval        2        0.75        2        4        5\n",
      "Probability        2        0.75        2        4        5\n",
      "Prediction        2        0.75        2        4        5\n",
      "Linear function        2        0.75        2        4        5\n",
      "Beta (finance)        2        0.75        2        4        5\n",
      "JSTOR        2        0.75        2        4        5\n",
      "Parameter (computer programming)        2        0.75        2        4        5\n",
      "System        2        0.75        2        4        5\n",
      "Parametric statistics        2        0.75        2        4        5\n",
      "Argument of a function        2        0.75        2        4        5\n",
      "Argument        2        0.75        2        4        5\n",
      "Computer programming        2        0.5        2        4        5\n",
      "Computer        2        0.5        2        4        5\n",
      "Semantics        2        0.5        2        4        5\n",
      "Formal language        2        0.5        2        4        5\n",
      "Program (machine)        2        0.5        2        4        5\n",
      "Java (programming language)        2        0.5        2        4        5\n",
      "Array programming        2        0.5        2        4        5\n",
      "Pascal (programming language)        2        0.5        2        4        5\n",
      "Data type        2        0.5        2        4        5\n",
      "Value (computer science)        2        0.5        2        4        5\n",
      "Matrix (mathematics)        2        0.5        2        4        5\n",
      "Memory address        2        0.75        2        4        5\n",
      "Void type        2        0.75        2        4        5\n",
      "Null pointer        2        0.75        2        4        5\n",
      "R (programming language)        2        0.375        2        4        5\n",
      "Lasso (statistics)        2        0.375        2        4        5\n",
      "MATLAB        2        0.375        2        4        5\n",
      "Support vector machine        2        0.375        2        4        5\n",
      "Logistic regression        2        0.375        2        4        5\n",
      "Apache Spark        2        0.375        2        4        5\n",
      "Proximal gradient method        2        0.75        2        4        5\n",
      "Total variation denoising        2        0.75        2        4        5\n",
      "Differentiable function        2        0.75        2        4        5\n",
      "Field norm        2        0.75        2        4        5\n",
      "Vector space        2        0.75        2        4        5\n",
      "Vector (mathematics and physics)        2        0.75        2        4        5\n",
      "F-space        2        0.75        2        4        5\n",
      "Normed vector space        2        0.75        2        4        5\n",
      "Topology        2        0.75        2        4        5\n",
      "Triangle inequality        2        0.75        2        4        5\n",
      "Linear filter        2        0.75        2        4        5\n",
      "Visual field        2        0.5        2        4        4\n",
      "Ganglion cell        2        0.75        2        4        5\n",
      "Optic tract        2        0.75        2        4        5\n",
      "Optic chiasm        2        0.75        2        4        5\n",
      "Optic nerve        2        0.75        2        4        5\n",
      "Wave        2        1.0        2        4        5\n",
      "Speed of light        2        1.0        2        4        5\n",
      "Wavelength        2        1.0        2        4        5\n",
      "Radiant flux        2        1.0        2        4        5\n",
      "Spectrum        2        1.0        2        4        5\n",
      "Units of measurement        2        1.0        2        4        5\n",
      "Optical medium        2        1.0        2        4        5\n",
      "Watt        2        1.0        2        4        5\n",
      "Intensity (physics)        2        1.0        2        4        5\n",
      "DNA        2        1.0        2        4        5\n",
      "Eukaryote        2        1.0        2        4        5\n",
      "Cell biology        2        1.0        2        4        5\n",
      "RNA        2        1.0        2        4        5\n",
      "Biology        2        1.0        2        4        5\n",
      "Protein        2        1.0        2        4        5\n",
      "Prokaryote        2        1.0        2        4        5\n",
      "Molecule        2        1.0        2        4        5\n",
      "Cell membrane        2        1.0        2        4        5\n",
      "Convolution        1        1.0        1        2        2\n",
      "Dropout (neural networks)        1        0.5        1        2        2\n",
      "Receptive field        1        1.0        1        2        2\n",
      "Surround suppression        1        0.5        1        3        3\n",
      "Lateral geniculate nucleus        1        0.5        1        3        3\n",
      "Auditory system        1        1.0        1        3        3\n",
      "C        1        1.0        1        3        3\n",
      "Unix        1        0.5        1        3        3\n",
      "For loop        1        0.5        1        3        3\n",
      "C99        1        0.5        1        3        3\n",
      "Compiler        1        0.5        1        3        3\n",
      "Impulse response        1        0.5        1        3        3\n",
      "Signal        1        1.0        1        3        3\n",
      "Convolution theorem        1        0.5        1        3        3\n",
      "Operation (mathematics)        1        1.0        1        3        3\n",
      "Support (mathematics)        1        1.0        1        3        3\n",
      "Support (measure theory)        1        1.0        1        4        4\n",
      "Compact space        1        1.0        1        4        4\n",
      "Topological space        1        1.0        1        4        4\n",
      "Real line        1        1.0        1        4        4\n",
      "Distribution (mathematics)        1        1.0        1        4        4\n",
      "Measure (mathematics)        1        1.0        1        4        4\n",
      "Euclidean space        1        1.0        1        4        4\n",
      "Closure (topology)        1        1.0        1        4        4\n",
      "Arity        1        1.0        1        4        4\n",
      "Binary operation        1        1.0        1        4        4\n",
      "Operand        1        1.0        1        4        4\n",
      "Unary operation        1        1.0        1        4        4\n",
      "Multiplication        1        1.0        1        4        4\n",
      "Discrete Fourier transform        1        0.5        1        4        4\n",
      "Asterisk        1        0.5        1        4        4\n",
      "Fubini's theorem        1        0.5        1        4        4\n",
      "Mellin transform        1        0.5        1        4        4\n",
      "Pontryagin duality        1        0.5        1        4        4\n",
      "Hartley transform        1        0.5        1        4        4\n",
      "Digital data        1        1.0        1        4        4\n",
      "Digital signal        1        1.0        1        4        4\n",
      "Analog signal        1        1.0        1        4        4\n",
      "Digital signal (signal processing)        1        1.0        1        4        4\n",
      "Cell signaling        1        1.0        1        4        4\n",
      "Dirac delta function        1        0.5        1        4        4\n",
      "Dynamical system        1        0.5        1        4        4\n",
      "Economics        1        0.5        1        4        4\n",
      "Convolution reverb        1        0.5        1        4        4\n",
      "Frequency response        1        0.5        1        4        4\n",
      "Optimizing compiler        1        0.5        1        4        4\n",
      "Source code        1        0.5        1        4        4\n",
      "Machine code        1        0.5        1        4        4\n",
      "Computer program        1        0.5        1        4        4\n",
      "Computer (magazine)        1        0.5        1        4        4\n",
      "Computer hardware        1        0.5        1        4        4\n",
      "GNU Compiler Collection        1        0.5        1        4        4\n",
      "Long double        1        0.5        1        4        4\n",
      "Library (computing)        1        0.5        1        4        4\n",
      "Microsoft        1        0.5        1        4        4\n",
      "Floating-point arithmetic        1        0.5        1        4        4\n",
      "X86        1        0.5        1        4        4\n",
      "IBM        1        0.5        1        4        4\n",
      "Microsoft Visual C++        1        0.5        1        4        4\n",
      "Equals sign        1        0.5        1        4        4\n",
      "Statement (computer science)        1        0.5        1        4        4\n",
      "Foreach loop        1        0.5        1        4        4\n",
      "Iteration        1        0.5        1        4        4\n",
      "Fortran        1        0.5        1        4        4\n",
      "The Open Group        1        0.5        1        4        4\n",
      "Linux        1        0.5        1        4        4\n",
      "Kernel (operating system)        1        0.5        1        4        4\n",
      "GNU        1        0.5        1        4        4\n",
      "Unix-like        1        0.5        1        4        4\n",
      "Berkeley Software Distribution        1        0.5        1        4        4\n",
      "Novell        1        0.5        1        4        4\n",
      "English language        1        1.0        1        4        4\n",
      "Latin        1        1.0        1        4        4\n",
      "Letter (alphabet)        1        1.0        1        4        4\n",
      "Latin alphabet        1        1.0        1        4        4\n",
      "Alphabet        1        1.0        1        4        4\n",
      "Phonetics        1        1.0        1        4        4\n",
      "Italian language        1        1.0        1        4        4\n",
      "G        1        1.0        1        4        4\n",
      "Sound        1        1.0        1        4        4\n",
      "Ear        1        1.0        1        4        4\n",
      "Cochlear nucleus        1        1.0        1        4        4\n",
      "Tectorial membrane        1        1.0        1        4        4\n",
      "Olivary body        1        1.0        1        4        4\n",
      "Hearing        1        1.0        1        4        4\n",
      "Hair cell        1        1.0        1        4        4\n",
      "Cochlea        1        1.0        1        4        4\n",
      "Superior colliculus        1        0.5        1        4        4\n",
      "Stimulus (psychology)        1        0.5        1        4        4\n",
      "Computer vision        1        0.5        1        4        4\n",
      "Electrophysiology        1        0.5        1        4        4\n",
      "Sensory nervous system        1        0.5        1        4        4\n"
     ]
    }
   ],
   "source": [
    "target_article = \"Convolutional neural network\"\n",
    "\n",
    "scored_edges = get_graph_edges(target_article, 3, 10)\n",
    "edges = [edge for edge,score in scored_edges]\n",
    "\n",
    "article_title = get_wiki_article_by_href(target_article).title\n",
    "\n",
    "#print(scored_edges)\n",
    "#print(edges)\n",
    "graph = get_graph(edges)\n",
    "\n",
    "bidir_probs, deeprank_probs, ns_paths, min_depths, max_depths = get_features(graph, article_title, 4, scored_edges)\n",
    "\n",
    "print(\"\\n\")\n",
    "for title, dr in sorted(deeprank_probs.items(), key=lambda a: a[1], reverse=True):\n",
    "    print(\"{0}        {1}        {2}        {3}        {4}        {5}\" \\\n",
    "          .format(title, round(dr,3), round(bidir_probs[title],3), ns_paths[title], min_depths[title], max_depths[title]))\n",
    "    #print(title, round(dr,3))\n",
    "\n",
    "save_everything()\n",
    "\n",
    "#MUST KEEP PLAYING WITH PARAMETERS\n",
    "#COME TO A CONCLUSION\n",
    "\n",
    "#CREATE MUSIC WITH MACHINE LEARNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(page_data['full'].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
