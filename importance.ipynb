{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedtree2\n",
    "Notebook to compile the so far research of nvgtt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib\n",
    "import networkx as nx\n",
    "\n",
    "from DictStorage import DictStorage\n",
    "from ThreadPool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page(page, lang=\"en\"):\n",
    "    \"\"\"Function to retrieve a wikipedia page in html form, with its sections\"\"\"\n",
    "\n",
    "    # https://en.wikipedia.org/w/api.php?action=parse&redirects&page=fluid_mechanics\n",
    "    \"\"\"\n",
    "    wikipediaApiUrl = \"https://\" + lang + \".wikipedia.org/w/api.php\"\n",
    "    \n",
    "    pageParams = {\n",
    "        'action': 'parse', \n",
    "        'redirects': True,\n",
    "        'page': urllib.unquote(page),\n",
    "        'format': 'json',\n",
    "        'prop':'text|displaytitle'\n",
    "    }\n",
    "\n",
    "    pageData = requests.get(wikipediaApiUrl, pageParams, timeout=0.001).json()\n",
    "    \"\"\"    \n",
    "    pageParams = [\n",
    "        'action=parse',\n",
    "        'redirects',\n",
    "        'format=json',\n",
    "        'prop=text|displaytitle',\n",
    "        'page=' + page\n",
    "    ]\n",
    "    \n",
    "    wikipediaApiUrl = \"https://\" + lang + \".wikipedia.org/w/api.php?\" + \"&\".join(pageParams)\n",
    "\n",
    "    pageData = requests.get(wikipediaApiUrl, timeout=60).json()\n",
    "    \n",
    "    if not 'parse' in pageData:\n",
    "        print(page) \n",
    "        print(urllib.unquote(page))\n",
    "        print(pageData)\n",
    "        raise \"Error while getting page \" + page\n",
    "\n",
    "\n",
    "    docHtml = BeautifulSoup(pageData['parse']['text']['*'], 'html.parser')\n",
    "\n",
    "    #Split document by its sections\n",
    "    docSections = __splitIntoSections__(docHtml)\n",
    "\n",
    "    structPageData = {\n",
    "        'title': pageData['parse']['title'],\n",
    "        'pageid': pageData['parse']['pageid'],\n",
    "        'full': docHtml,\n",
    "        'sections': docSections\n",
    "    }\n",
    "\n",
    "    return structPageData\n",
    "\n",
    "\n",
    "def __splitIntoSections__(htmlObj):\n",
    "    \"\"\"Function to split html document in sections (use h2 tags as divisors)\"\"\"\n",
    "\n",
    "    #Init var to store sections\n",
    "    sectionObjs = [[]]\n",
    "\n",
    "    for tag in htmlObj.children:\n",
    "        #Start new section in case the tag is h2\n",
    "        if tag.name == 'h2':\n",
    "            sectionObjs.append([])\n",
    "\n",
    "        #If it is a valid tag (invalid tags has no 'name' property)\n",
    "        if tag.name != None:\n",
    "            sectionObjs[len(sectionObjs) - 1].append(tag)\n",
    "\n",
    "    return sectionObjs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print get_page(\"C%2B%2B\")['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page_and_parse(page):\n",
    "    \"\"\"Function to treat the data, remove unecessary things etc.\"\"\"\n",
    "    \n",
    "    page_data = get_page(page)\n",
    "    \n",
    "    soup = page_data['full']\n",
    "        \n",
    "    #Clear table of contents if any\n",
    "    for node in soup.findAll(id='toc'):\n",
    "        node.decompose()\n",
    "\n",
    "    #Clear top info table if any\n",
    "    for node in soup.findAll(class_='ambox'):\n",
    "        node.decompose()\n",
    "\n",
    "    #Clear info box if any\n",
    "    for node in soup.findAll(class_='infobox'):\n",
    "        node.decompose()\n",
    "\n",
    "    #Clear verticalbox if any\n",
    "    for node in soup.findAll(class_='vertical-navbox'):\n",
    "        node.decompose()\n",
    "        \n",
    "    #Clear navbox if any\n",
    "    for node in soup.findAll(class_='navbox'):\n",
    "        node.decompose()\n",
    "        \n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blocked_link_terms = set([\n",
    "    \"(disambiguation)\", #Not interested in disambiguation pages\n",
    "    \":\" #Pages with colon are offen special pages. Not sure if there is articles with colon\n",
    "])\n",
    "\n",
    "def get_page_links(page_data):\n",
    "    \"\"\"Function to get links \"\"\"\n",
    "    #page_data = get_page_and_parse(page)\n",
    "    \n",
    "    links = list()\n",
    "    \n",
    "    for link in page_data['full'].findAll(\"a\"):\n",
    "        \n",
    "        #If the a tag has no href attr, skip it\n",
    "        if not link.has_attr(\"href\"):\n",
    "            continue\n",
    "            \n",
    "        #If the href does not starts with \"/wiki/\", skip it\n",
    "        if link['href'].find(\"/wiki/\") != 0:\n",
    "            continue\n",
    "            \n",
    "        #Check if some blocked term is present in the href, if so, skip the link\n",
    "        skip_link = False\n",
    "        for term in blocked_link_terms:\n",
    "            if link['href'].find(term) != -1:\n",
    "                skip_link = True\n",
    "                break\n",
    "        if skip_link:\n",
    "            continue\n",
    "        \n",
    "        #Get only the link portion\n",
    "        #We MUST NOT use last index of / to get the path cause some titles like TCP/IP, have bar in the title\n",
    "        #We should use the '/wiki/' string length\n",
    "        linkHref = link['href'][6:]\n",
    "        \n",
    "        #Remove hashtag from url if any\n",
    "        hashIndex = linkHref.find(\"#\")\n",
    "        if hashIndex != -1:\n",
    "            linkHref = linkHref[:hashIndex]\n",
    "            \n",
    "        linkText = link.get_text()\n",
    "            \n",
    "        links.append((linkHref, linkText))\n",
    "            \n",
    "    return links#, n_valid_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikisyn = DictStorage(\"wikisyn\") #Storage for link synoms\n",
    "href_to_pageid = DictStorage(\"href_to_pageid\") #Storage lookup table of hrefs and pageids\n",
    "pageid_to_title = DictStorage(\"pageid_to_title\") #Storage for page titles\n",
    "pageid_to_href = DictStorage(\"pageid_to_href\") #Storage lookup table of pageid and hrefs\n",
    "pageid_to_page_links = DictStorage(\"pageid_to_page_links\") #Storage for page links\n",
    "pageid_to_page_text = DictStorage(\"pageid_to_page_text\") #Storage for page texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to save page data to storages\n",
    "def save_page_data(page_href, page_data):\n",
    "    page_title = page_data['title']\n",
    "    page_id = page_data['pageid']\n",
    "    page_text = page_data['full'].get_text()\n",
    "    \n",
    "    #Register page id lookup tables\n",
    "    href_to_pageid[page_href] = page_id\n",
    "    href_to_pageid[page_title] = page_id\n",
    "    \n",
    "    #Register page title\n",
    "    pageid_to_title[page_id] = page_title\n",
    "    \n",
    "    if not page_id in pageid_to_href:\n",
    "        pageid_to_href[page_id] = set()\n",
    "    pageid_to_href[page_id].add(page_href)\n",
    "    pageid_to_href[page_id].add(page_title)\n",
    "    \n",
    "    #Register page text\n",
    "    pageid_to_page_text[page_id] = page_text\n",
    "    \n",
    "    #Register page links and wikisyn\n",
    "    page_links = get_page_links(page_data)\n",
    "    pageid_to_page_links[page_id] = set()\n",
    "    for link_href, link_text in page_links:\n",
    "        pageid_to_page_links[page_id].add(link_href)\n",
    "        if not link_href in wikisyn:\n",
    "            wikisyn[link_href] = set()\n",
    "        wikisyn[link_href].add(link_text)\n",
    "       \n",
    "    #Save everything\n",
    "    href_to_pageid.save()\n",
    "    pageid_to_title.save()\n",
    "    pageid_to_href.save()\n",
    "    pageid_to_page_text.save()\n",
    "    pageid_to_page_links.save()\n",
    "    wikisyn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pageid(page):\n",
    "    \n",
    "    #Check if the page is in the redirects table, if not, download it and register it\n",
    "    if not page in href_to_pageid:\n",
    "        print(\"Page not found. Downloading and registering it...\")\n",
    "        page_data = get_page_and_parse(page)\n",
    "        save_page_data(page, page_data)\n",
    "        print(\"Done.\")\n",
    "        \n",
    "    return href_to_pageid[page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to download bunch of wikipedia pages at once if they are not present\n",
    "check_and_download__done = 0\n",
    "def check_and_download(pages):\n",
    "    global check_and_download__done\n",
    "    check_and_download__done = 0\n",
    "    \n",
    "    n_tasks = len(pages)\n",
    "    \n",
    "    # Function to be executed in a thread\n",
    "    def download_stuff(page):\n",
    "        global check_and_download__done\n",
    "        try:\n",
    "            get_pageid(page)\n",
    "            check_and_download__done += 1\n",
    "            #print(\"Done \" + str(check_and_download__done) + \"/\" + str(n_tasks))\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Failed to get page \" + page + \". Timed out.\")\n",
    "\n",
    "    # Instantiate a thread pool with 5 worker threads\n",
    "    pool = ThreadPool(10)\n",
    "\n",
    "    pool.map(download_stuff, pages)\n",
    "    pool.wait_completion()\n",
    "    \n",
    "    print(\"Finishing downloading. Done tasks: \" + str(check_and_download__done) + \"/\" + str(n_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check_and_download([\"Node.js\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#target_page = urllib.quote(\"JavaScript\")\n",
    "#target_id = get_pageid(target_page)\n",
    "#target_links = pageid_to_page_links[target_id]\n",
    "#target_links_ids = set()\n",
    "\n",
    "#for i, link in enumerate(target_links):\n",
    "    #print(\"Working on \" + link + \". \" + str((i+1)) + \"/\" + str(len(target_links)))\n",
    "    #target_links_ids.add(get_pageid(link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_sorted_list(data, key, reverse=False):\n",
    "    for k, v in sorted(data, key=key, reverse=reverse):\n",
    "        print(k,v)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links_score(page):\n",
    "    \"\"\"Function to cross a list of links with a text, setting scores.\"\"\"\n",
    "    \n",
    "    pageid = href_to_pageid[page]\n",
    "    page_links = pageid_to_page_links[pageid]\n",
    "    page_text = pageid_to_page_text[pageid]\n",
    "    \n",
    "    links_score = dict()\n",
    "    \n",
    "    norm_fact = 0\n",
    "    \n",
    "    #Ensure all page links are present\n",
    "    check_and_download(page_links)\n",
    "    \n",
    "    for link_href in page_links:\n",
    "        links_score[link_href] = 0\n",
    "        for l_text in wikisyn[link_href]:\n",
    "            matches = re.findall('[^a-zA-Z0-9_]' + re.escape(l_text) + '[^a-zA-Z0-9_]', page_text, re.IGNORECASE)\n",
    "            links_score[link_href] += len(matches) \n",
    "            norm_fact += len(matches)\n",
    "            \n",
    "    norm_links_score = dict(map(lambda a: [a[0], float(a[1])/norm_fact], links_score.items()))\n",
    "            \n",
    "    return norm_links_score\n",
    "\n",
    "#v_sum = 0\n",
    "#links_score = get_links_score(\"MQTT\")\n",
    "#for k, v in sorted(links_score.items(), key=lambda a:a[1], reverse=True):\n",
    "    #print(k,wikisyn[k],v)\n",
    "    #v_sum += v\n",
    "#print v_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing downloading. Done tasks: 74/74\n",
      "('AutoCAD', 'Autodesk') 0.1544502617801047\n",
      "('AutoCAD', 'Macintosh') 0.11518324607329843\n",
      "('AutoCAD', '.dwg') 0.07329842931937172\n",
      "('AutoCAD', 'MacOS') 0.07329842931937172\n",
      "('AutoCAD', 'Microsoft Windows') 0.06806282722513089\n",
      "('AutoCAD', 'Computer-aided design') 0.05235602094240838\n",
      "('AutoCAD', 'Cloud computing') 0.03664921465968586\n",
      "('AutoCAD', 'Commercial software') 0.031413612565445025\n",
      "('AutoCAD', 'Android (operating system)') 0.028795811518324606\n",
      "('AutoCAD', 'Portable Document Format') 0.020942408376963352\n",
      "('AutoCAD', 'Italian language') 0.01832460732984293\n",
      "('AutoCAD', 'IOS') 0.01832460732984293\n",
      "('AutoCAD', 'Architecture') 0.015706806282722512\n",
      "('AutoCAD', 'Mac App Store') 0.015706806282722512\n",
      "('AutoCAD', 'Mobile app') 0.015706806282722512\n",
      "('AutoCAD', 'App Store (iOS)') 0.015706806282722512\n",
      "('AutoCAD', 'Dexigner') 0.015706806282722512\n",
      "('AutoCAD', 'AutoCAD Architecture') 0.013089005235602094\n",
      "('AutoCAD', 'Mac OS X Lion') 0.013089005235602094\n",
      "('AutoCAD', 'Design Web Format') 0.007853403141361256\n",
      "('AutoCAD', 'DGN') 0.007853403141361256\n",
      "('AutoCAD', 'Mainframe computer') 0.007853403141361256\n",
      "('AutoCAD', 'Ribbon (computing)') 0.007853403141361256\n",
      "('AutoCAD', '.NET Framework') 0.005235602094240838\n",
      "('AutoCAD', 'Wayback Machine') 0.005235602094240838\n",
      "('AutoCAD', 'MS-DOS') 0.005235602094240838\n",
      "('AutoCAD', 'IPod Touch') 0.005235602094240838\n",
      "('AutoCAD', 'AutoLISP') 0.005235602094240838\n",
      "('AutoCAD', 'Freemium') 0.005235602094240838\n",
      "('AutoCAD', 'Google Play') 0.005235602094240838\n",
      "('AutoCAD', 'Autodesk 3ds Max') 0.005235602094240838\n",
      "('AutoCAD', 'German language') 0.005235602094240838\n",
      "('AutoCAD', 'AutoCAD DXF') 0.005235602094240838\n",
      "('AutoCAD', 'Autodesk Maya') 0.005235602094240838\n",
      "('AutoCAD', 'ObjectARX') 0.005235602094240838\n",
      "('AutoCAD', 'IPad') 0.005235602094240838\n",
      "('AutoCAD', 'Unix') 0.005235602094240838\n",
      "('AutoCAD', 'Visual Basic for Applications') 0.005235602094240838\n",
      "('AutoCAD', 'IPhone') 0.005235602094240838\n",
      "('AutoCAD', 'Autodesk Revit') 0.005235602094240838\n",
      "('AutoCAD', 'C++') 0.002617801047120419\n",
      "('AutoCAD', 'English language') 0.002617801047120419\n",
      "('AutoCAD', 'Traditional Chinese characters') 0.002617801047120419\n",
      "('AutoCAD', 'Comparison of computer-aided design editors') 0.002617801047120419\n",
      "('AutoCAD', 'Technical drawing') 0.002617801047120419\n",
      "('AutoCAD', 'Minicomputer') 0.002617801047120419\n",
      "('AutoCAD', 'Web application') 0.002617801047120419\n",
      "('AutoCAD', 'COMDEX') 0.002617801047120419\n",
      "('AutoCAD', 'De facto') 0.002617801047120419\n",
      "('AutoCAD', 'Simplified Chinese characters') 0.002617801047120419\n",
      "('AutoCAD', 'Windows 3.1x') 0.002617801047120419\n",
      "('AutoCAD', 'Amazon Appstore') 0.002617801047120419\n",
      "('AutoCAD', 'Hungarian language') 0.002617801047120419\n",
      "('AutoCAD', 'Microcomputer') 0.002617801047120419\n",
      "('AutoCAD', 'Spanish language') 0.002617801047120419\n",
      "('AutoCAD', 'French language') 0.002617801047120419\n",
      "('AutoCAD', 'HTML5') 0.002617801047120419\n",
      "('AutoCAD', 'Polish language') 0.002617801047120419\n",
      "('AutoCAD', 'Albanian language') 0.002617801047120419\n",
      "('AutoCAD', 'Application programming interface') 0.002617801047120419\n",
      "('AutoCAD', 'AutoSketch') 0.002617801047120419\n",
      "('AutoCAD', 'Russian language') 0.002617801047120419\n",
      "('AutoCAD', 'Dropbox (service)') 0.002617801047120419\n",
      "('AutoCAD', 'Data conversion') 0.002617801047120419\n",
      "('AutoCAD', 'Computer terminal') 0.002617801047120419\n",
      "('AutoCAD', 'Brazilian Portuguese') 0.002617801047120419\n",
      "('AutoCAD', 'Korean language') 0.002617801047120419\n",
      "('AutoCAD', 'AutoShade') 0.002617801047120419\n",
      "('AutoCAD', 'Video card') 0.002617801047120419\n",
      "('AutoCAD', 'Interoperability') 0.002617801047120419\n",
      "('AutoCAD', 'Czech language') 0.002617801047120419\n",
      "('AutoCAD', 'List of XML markup languages') 0.002617801047120419\n"
     ]
    }
   ],
   "source": [
    "def get_node_edges_scores(page_href):\n",
    "    \"\"\"Function to get node edges to be placed in the graph. \"\"\"\n",
    "    \n",
    "    edges = dict()\n",
    "    \n",
    "    #Get main page data\n",
    "    page_id = get_pageid(page_href)\n",
    "    page_title = pageid_to_title[page_id]\n",
    "    page_links = get_links_score(page_href).items()\n",
    "    \n",
    "    for i, (link_href, score) in enumerate(page_links):\n",
    "        #print(\"Working on link {0} {1}/{2}\".format(link_href, i+1, len(page_links)))\n",
    "        link_id = get_pageid(link_href)\n",
    "        link_title = pageid_to_title[link_id]\n",
    "        \n",
    "        #If there is already a title already place, sum the scores\n",
    "        if (page_title, link_title) in edges:\n",
    "            edges[(page_title, link_title)] += score\n",
    "        else:\n",
    "            edges[(page_title, link_title)] = score        \n",
    "        \n",
    "    return edges\n",
    "    \n",
    "\n",
    "edges_scores = get_node_edges_scores(\"AutoCAD\")\n",
    "print_sorted_list(edges_scores.items(), lambda a:a[1], True)\n",
    "\n",
    "#CREATE METHOD TO CREATE GRAPH BASED ON DEEPNESS\n",
    "#MAYBE PLACE STOP CONDITION TO NOT DOWNLOAD EVERY LINK\n",
    "#CHECK WHETHER WIKISYN IS REALLY GOOD BECAUSE OF ERRORS. MAYBE KEEP TRACK HOW MANY TIMES EACH WORD APPEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#links_score = get_page_links_score(page_links, page_data['full'].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#total_links = 0\n",
    "#for k, v in links_score.items():\n",
    "    #total_links += v\n",
    "\n",
    "#for k, v in sorted(links_score.items(), key=lambda a:a[1], reverse=True):\n",
    "    #print(k,page_links[k],float(v),float(v)/total_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#page_data = get_links_score()\n",
    "#for d in page_data.iteritems():\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(page_data['full'].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
