{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Embedtree2\n",
    "Notebook to compile the so far research of nvgtt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "import urllib\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Dict Storage class to be used to load and save dicts\n",
    "It saves data in pickle format and works with lower case only to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DictStorage(dict):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename + \".pickle\"\n",
    "        \n",
    "        try:\n",
    "            with open(self.filename, mode='r') as pickle_file:\n",
    "                dict_list = pickle.load(pickle_file)\n",
    "        except IOError:\n",
    "            dict_list = []\n",
    "            print(\"Failed to open \" + self.filename + \". Created empty dict.\")\n",
    "        finally:\n",
    "            dict.__init__(self, dict_list)\n",
    "    \n",
    "    def save(self):\n",
    "        with open(self.filename, mode='w') as pickle_file:\n",
    "            pickle.dump(self.items(), pickle_file)\n",
    "            \n",
    "  #  def __setitem__(self, key, value):\n",
    " #       #Ensure data is string \n",
    " #       super(DictStorage, self).__setitem__(key.lower(), value)\n",
    "\n",
    " #   def __getitem__(self, key):\n",
    "#        #Ensure data is string \n",
    " #       return super(DictStorage, self).__getitem__(key.lower())\n",
    "    \n",
    "#    def has_key(self, key):\n",
    "#        #Ensure data is string \n",
    "#        return super(DictStorage, self).has_key(key.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_dictstorage():\n",
    "    test1 = DictStorage(\"ae2\")\n",
    "    print(test1.items())\n",
    "    print(test1.save())\n",
    "    test1['LUCAS'] = 128\n",
    "    print(test1.items())\n",
    "    print(test1.save())\n",
    "    print(test1['lucas'])\n",
    "    print(test1['LUCAS'])\n",
    "    \n",
    "#test_dictstorage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Function to get wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_page(page, lang=\"en\"):\n",
    "    \"\"\"Function to retrieve a wikipedia page in html form, with its sections\"\"\"\n",
    "\n",
    "    # https://en.wikipedia.org/w/api.php?action=parse&redirects&page=fluid_mechanics\n",
    "    \"\"\"\n",
    "    wikipediaApiUrl = \"https://\" + lang + \".wikipedia.org/w/api.php\"\n",
    "    \n",
    "    pageParams = {\n",
    "        'action': 'parse', \n",
    "        'redirects': True,\n",
    "        'page': urllib.unquote(page),\n",
    "        'format': 'json',\n",
    "        'prop':'text|displaytitle'\n",
    "    }\n",
    "\n",
    "    pageData = requests.get(wikipediaApiUrl, pageParams).json()\n",
    "    \"\"\"    \n",
    "    pageParams = [\n",
    "        'action=parse',\n",
    "        'redirects',\n",
    "        'format=json',\n",
    "        'prop=text|displaytitle',\n",
    "        'page=' + page\n",
    "    ]\n",
    "    \n",
    "    wikipediaApiUrl = \"https://\" + lang + \".wikipedia.org/w/api.php?\" + \"&\".join(pageParams)\n",
    "\n",
    "    pageData = requests.get(wikipediaApiUrl).json()\n",
    "    \n",
    "    if not 'parse' in pageData:\n",
    "        print page \n",
    "        print urllib.unquote(page)\n",
    "        print(pageData)\n",
    "        raise \"Error while getting page \" + page\n",
    "\n",
    "\n",
    "    docHtml = BeautifulSoup(pageData['parse']['text']['*'], 'html.parser')\n",
    "\n",
    "    #Split document by its sections\n",
    "    docSections = __splitIntoSections__(docHtml)\n",
    "\n",
    "    structPageData = {\n",
    "        'title': pageData['parse']['title'],\n",
    "        'pageid': pageData['parse']['pageid'],\n",
    "        'full': docHtml,\n",
    "        'sections': docSections\n",
    "    }\n",
    "\n",
    "    return structPageData\n",
    "\n",
    "\n",
    "def __splitIntoSections__(htmlObj):\n",
    "    \"\"\"Function to split html document in sections (use h2 tags as divisors)\"\"\"\n",
    "\n",
    "    #Init var to store sections\n",
    "    sectionObjs = [[]]\n",
    "\n",
    "    for tag in htmlObj.children:\n",
    "        #Start new section in case the tag is h2\n",
    "        if tag.name == 'h2':\n",
    "            sectionObjs.append([])\n",
    "\n",
    "        #If it is a valid tag (invalid tags has no 'name' property)\n",
    "        if tag.name != None:\n",
    "            sectionObjs[len(sectionObjs) - 1].append(tag)\n",
    "\n",
    "    return sectionObjs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print get_page(\"C%2B%2B\")['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_page_and_parse(page):\n",
    "    \"\"\"Function to treat the data, remove unecessary things etc.\"\"\"\n",
    "    \n",
    "    page_data = get_page(page)\n",
    "    \n",
    "    soup = page_data['full']\n",
    "        \n",
    "    #Clear table of contents if any\n",
    "    for node in soup.findAll(id='toc'):\n",
    "        node.decompose()\n",
    "\n",
    "    #Clear top info table if any\n",
    "    for node in soup.findAll(class_='ambox'):\n",
    "        node.decompose()\n",
    "\n",
    "    #Clear info box if any\n",
    "    for node in soup.findAll(class_='infobox'):\n",
    "        node.decompose()\n",
    "\n",
    "    #Clear verticalbox if any\n",
    "    for node in soup.findAll(class_='vertical-navbox'):\n",
    "        node.decompose()\n",
    "        \n",
    "    #Clear navbox if any\n",
    "    for node in soup.findAll(class_='navbox'):\n",
    "        node.decompose()\n",
    "        \n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "blocked_link_terms = set([\n",
    "    \"(disambiguation)\", #Not interested in disambiguation pages\n",
    "    \":\" #Pages with colon are offen special pages. Not sure if there is articles with colon\n",
    "])\n",
    "\n",
    "def get_page_links(page_data):\n",
    "    \"\"\"Function to get links \"\"\"\n",
    "    #page_data = get_page_and_parse(page)\n",
    "    \n",
    "    links = list()\n",
    "    \n",
    "    for link in page_data['full'].findAll(\"a\"):\n",
    "        \n",
    "        #If the a tag has no href attr, skip it\n",
    "        if not link.has_attr(\"href\"):\n",
    "            continue\n",
    "            \n",
    "        #If the href does not starts with \"/wiki/\", skip it\n",
    "        if link['href'].find(\"/wiki/\") != 0:\n",
    "            continue\n",
    "            \n",
    "        #Check if some blocked term is present in the href, if so, skip the link\n",
    "        skip_link = False\n",
    "        for term in blocked_link_terms:\n",
    "            if link['href'].find(term) != -1:\n",
    "                skip_link = True\n",
    "                break\n",
    "        if skip_link:\n",
    "            continue\n",
    "        \n",
    "        #Get only the link portion\n",
    "        #We MUST NOT use last index of / to get the path cause some titles like TCP/IP, have bar in the title\n",
    "        #We should use the '/wiki/' string length\n",
    "        linkHref = link['href'][6:]\n",
    "        \n",
    "        #Remove hashtag from url if any\n",
    "        hashIndex = linkHref.find(\"#\")\n",
    "        if hashIndex != -1:\n",
    "            linkHref = linkHref[:hashIndex]\n",
    "            \n",
    "        linkText = link.get_text()\n",
    "            \n",
    "        links.append((linkHref, linkText))\n",
    "            \n",
    "    return links#, n_valid_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikisyn = DictStorage(\"wikisyn\") #Storage for link synoms\n",
    "href_to_pageid = DictStorage(\"href_to_pageid\") #Storage lookup table of hrefs and pageids\n",
    "pageid_to_title = DictStorage(\"pageid_to_title\") #Storage for page titles\n",
    "pageid_to_href = DictStorage(\"pageid_to_href\") #Storage lookup table of pageid and hrefs\n",
    "pageid_to_page_links = DictStorage(\"pageid_to_page_links\") #Storage for page links\n",
    "pageid_to_page_text = DictStorage(\"pageid_to_page_text\") #Storage for page texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Function to save page data to storages\n",
    "def save_page_data(page_href, page_data):\n",
    "    page_title = page_data['title']\n",
    "    page_id = page_data['pageid']\n",
    "    page_text = page_data['full'].get_text()\n",
    "    \n",
    "    #Register page id lookup tables\n",
    "    href_to_pageid[page_href] = page_id\n",
    "    href_to_pageid[page_title] = page_id\n",
    "    \n",
    "    #Register page title\n",
    "    pageid_to_title[page_id] = page_title\n",
    "    \n",
    "    if not pageid_to_href.has_key(page_id):\n",
    "        pageid_to_href[page_id] = set()\n",
    "    pageid_to_href[page_id].add(page_href)\n",
    "    pageid_to_href[page_id].add(page_title)\n",
    "    \n",
    "    #Register page text\n",
    "    pageid_to_page_text[page_id] = page_text\n",
    "    \n",
    "    #Register page links and wikisyn\n",
    "    page_links = get_page_links(page_data)\n",
    "    pageid_to_page_links[page_id] = set()\n",
    "    for link_href, link_text in page_links:\n",
    "        pageid_to_page_links[page_id].add(link_href)\n",
    "        if not wikisyn.has_key(link_href):\n",
    "            wikisyn[link_href] = set()\n",
    "        wikisyn[link_href].add(link_text)\n",
    "       \n",
    "    #Save everything\n",
    "    href_to_pageid.save()\n",
    "    pageid_to_title.save()\n",
    "    pageid_to_href.save()\n",
    "    pageid_to_page_text.save()\n",
    "    pageid_to_page_links.save()\n",
    "    wikisyn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_pageid(page):\n",
    "    \n",
    "    #Check if the page is in the redirects table, if not, download it and register it\n",
    "    if not href_to_pageid.has_key(page):\n",
    "        print(\"Page not found. Downloading and registering it...\")\n",
    "        page_data = get_page_and_parse(page)\n",
    "        save_page_data(page, page_data)\n",
    "        print(\"Done.\")\n",
    "        \n",
    "    return href_to_pageid[page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#target_page = urllib.quote(\"JavaScript\")\n",
    "#target_id = get_pageid(target_page)\n",
    "#target_links = pageid_to_page_links[target_id]\n",
    "#target_links_ids = set()\n",
    "#MUST FIND HOW TO DEAL WITH  NETWORKX NODES NAMES\n",
    "#WE WILL NEED TO ALWAYS DOWNLOAD ONE DEEP MORE TO GET LINKS ID INFOS\n",
    "\n",
    "#MAYBE KEEP TRACK OF ARTICLES TITLES\n",
    "#for i, link in enumerate(target_links):\n",
    "    #print(\"Working on \" + link + \". \" + str((i+1)) + \"/\" + str(len(target_links)))\n",
    "    #target_links_ids.add(get_pageid(link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_sorted_list(data, key, reverse=False):\n",
    "    for k, v in sorted(data, key=key, reverse=reverse):\n",
    "        print(k,v)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_links_score(page):\n",
    "    \"\"\"Function to cross a list of links with a text, setting scores.\"\"\"\n",
    "    \n",
    "    pageid = href_to_pageid[page]\n",
    "    page_links = pageid_to_page_links[pageid]\n",
    "    page_text = pageid_to_page_text[pageid]\n",
    "    \n",
    "    links_score = dict()\n",
    "    \n",
    "    norm_fact = 0\n",
    "    \n",
    "    for link_href in page_links:\n",
    "        links_score[link_href] = 0\n",
    "        for l_text in wikisyn[link_href]:\n",
    "            matches = re.findall('[^a-zA-Z0-9_]' + re.escape(l_text) + '[^a-zA-Z0-9_]', page_text, re.IGNORECASE)\n",
    "            links_score[link_href] += len(matches) \n",
    "            norm_fact += len(matches)\n",
    "            \n",
    "    norm_links_score = dict(map(lambda a: [a[0], float(a[1])/norm_fact], links_score.items()))\n",
    "            \n",
    "    return norm_links_score\n",
    "\n",
    "#v_sum = 0\n",
    "#links_score = get_links_score(\"MQTT\")\n",
    "#for k, v in sorted(links_score.items(), key=lambda a:a[1], reverse=True):\n",
    "    #print(k,wikisyn[k],v)\n",
    "    #v_sum += v\n",
    "#print v_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((u'MQTT', u'Internet protocol suite'), 0.09090909090909091)\n",
      "((u'MQTT', u'Publish\\u2013subscribe pattern'), 0.06818181818181818)\n",
      "((u'MQTT', u'Messaging pattern'), 0.06818181818181818)\n",
      "((u'MQTT', u'Facebook Messenger'), 0.06818181818181818)\n",
      "((u'MQTT', u'Adafruit Industries'), 0.06818181818181818)\n",
      "((u'MQTT', u'Message queue'), 0.06818181818181818)\n",
      "((u'MQTT', u'OASIS (organization)'), 0.06818181818181818)\n",
      "((u'MQTT', u'Constrained Application Protocol'), 0.045454545454545456)\n",
      "((u'MQTT', u'Advanced Message Queuing Protocol'), 0.045454545454545456)\n",
      "((u'MQTT', u'XMPP'), 0.045454545454545456)\n",
      "((u'MQTT', u'Online chat'), 0.045454545454545456)\n",
      "((u'MQTT', u'Service-oriented architecture'), 0.045454545454545456)\n",
      "((u'MQTT', u'OpenStack'), 0.045454545454545456)\n",
      "((u'MQTT', u'Machine to machine'), 0.022727272727272728)\n",
      "((u'MQTT', u'SensorThings API'), 0.022727272727272728)\n",
      "((u'MQTT', u'Amazon Web Services'), 0.022727272727272728)\n",
      "((u'MQTT', u'Andy Stanford-Clark'), 0.022727272727272728)\n",
      "((u'MQTT', u'EVRYTHNG'), 0.022727272727272728)\n",
      "((u'MQTT', u'Open Geospatial Consortium'), 0.022727272727272728)\n",
      "((u'MQTT', u'Web Application Messaging Protocol'), 0.022727272727272728)\n",
      "((u'MQTT', u'GitHub'), 0.022727272727272728)\n",
      "((u'MQTT', u'Message broker'), 0.022727272727272728)\n",
      "((u'MQTT', u'ZigBee'), 0.022727272727272728)\n"
     ]
    }
   ],
   "source": [
    "def get_node_edges_scores(page_href):\n",
    "    \"\"\"Function to get node edges to be placed in the graph. \"\"\"\n",
    "    \n",
    "    edges = dict()\n",
    "    \n",
    "    #Get main page data\n",
    "    page_id = get_pageid(page_href)\n",
    "    page_title = pageid_to_title[page_id]\n",
    "    page_links = get_links_score(page_href).items()\n",
    "    \n",
    "    for i, (link_href, score) in enumerate(page_links):\n",
    "        #print(\"Working on link {0} {1}/{2}\".format(link_href, i+1, len(page_links)))\n",
    "        link_id = get_pageid(link_href)\n",
    "        link_title = pageid_to_title[link_id]\n",
    "        \n",
    "        #If there is already a title already place, sum the scores\n",
    "        if edges.has_key((page_title, link_title)):\n",
    "            edges[(page_title, link_title)] += score\n",
    "        else:\n",
    "            edges[(page_title, link_title)] = score        \n",
    "        \n",
    "    return edges\n",
    "    \n",
    "\n",
    "edges_scores = get_node_edges_scores(\"MQTT\")\n",
    "print_sorted_list(edges_scores.items(), lambda a:a[1], True)\n",
    "\n",
    "MUST CREATE WAY TO DOWNLOAD DATA FASTER (ASYNC STUFF)\n",
    "CREATE METHOD TO CREATE GRAPH BASED ON DEEPNESS\n",
    "MAYBE PLACE STOP CONDITION TO NOT DOWNLOAD EVERY LINK\n",
    "CHECK WHETHER WIKISYN IS REALLY GOOD BECAUSE OF ERRORS. MAYBE KEEP TRACK HOW MANY TIMES EACH WORD APPEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#links_score = get_page_links_score(page_links, page_data['full'].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#total_links = 0\n",
    "#for k, v in links_score.items():\n",
    "    #total_links += v\n",
    "\n",
    "#for k, v in sorted(links_score.items(), key=lambda a:a[1], reverse=True):\n",
    "    #print(k,page_links[k],float(v),float(v)/total_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#page_data = get_links_score()\n",
    "#for d in page_data.iteritems():\n",
    "    #print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(page_data['full'].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
