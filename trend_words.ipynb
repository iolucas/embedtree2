{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wikipydia import dict_storage, wikidb, parse, wikisyn\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union(set(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_db = wikidb.WikiDb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "def get_article_sections(title):\n",
    "    \n",
    "    article, _ = wiki_db.get_article_by_href(quote(title))\n",
    "    \n",
    "    html_sections = parse._split_html_h2_sections(article.html())\n",
    "    text_sections = list()\n",
    "    \n",
    "    for html_sec in html_sections:\n",
    "        text_sections.append(parse._get_html_text(html_sec).replace(\"\\n\", \"\"))\n",
    "        \n",
    "    return text_sections\n",
    "\n",
    "#get_article_sections(\"NodeMCU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(text, window=1):\n",
    "    tokens = list()\n",
    "    for word in word_tokenize(text.lower()):\n",
    "        if word not in stop_words:\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_wikipedia(term, lang=\"en\", timeout=60):\n",
    "    \"\"\"Function search articles on wikipedia.\"\"\"\n",
    "    #import exceptions\n",
    "\n",
    "    #https://en.wikipedia.org/w/api.php?action=query&utf8&list=search&srsearch=neural\n",
    "#https://www.mediawiki.org/wiki/API:Search\n",
    "\n",
    "    # https://en.wikipedia.org/w/api.php?action=parse&redirects&page=fluid_mechanics\n",
    "\n",
    "    req_params = [\n",
    "        'action=query',\n",
    "        'utf8',\n",
    "        'list=search',\n",
    "        'format=json',\n",
    "        #'srwhat=text',\n",
    "        'srprop',\n",
    "        'srlimit=500',\n",
    "        'srsearch=' + term\n",
    "    ]\n",
    "\n",
    "    wikipedia_api_url = \"https://\" + lang + \".wikipedia.org/w/api.php?\" + \"&\".join(req_params)\n",
    "\n",
    "    page_data = requests.get(wikipedia_api_url, timeout=timeout).json()\n",
    "\n",
    "    results = [result['title'] for result in page_data['query']['search']]\n",
    "    \n",
    "    return results\n",
    "\n",
    "#search_wikipedia(\"neural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recommend_articles(text_content, n_tokens=10):\n",
    "    tokens = Counter(get_tokens(text_content))\n",
    "    \n",
    "    search_results = Counter()\n",
    "\n",
    "    i = 0\n",
    "    for word, count in tokens.most_common(n_tokens):\n",
    "        search_results.update(search_wikipedia(word))\n",
    "        i+=1\n",
    "        print(\"Done {0}  \".format(i), end='')\n",
    "\n",
    "    return search_results\n",
    "#print(search_results.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sections = get_article_sections(\"Deep_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning — deep library source open cpus provides python parallelization\n"
     ]
    }
   ],
   "source": [
    "tokens = Counter(get_tokens(sections[11])).most_common(10)\n",
    "print(\" \".join(word for word, count in tokens))\n",
    "#https://www.microsoft.com/cognitive-services/en-us/bing-web-search-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on section 1 of 6.\n",
      "Done 1  Done 2  Done 3  Done 4  Done 5  Done 6  Done 7  Done 8  Done 9  Done 10  Working on section 2 of 6.\n",
      "Done 1  Done 2  Done 3  Done 4  Done 5  Done 6  Done 7  Done 8  Done 9  Done 10  Working on section 3 of 6.\n",
      "Done 1  Done 2  Done 3  Done 4  Done 5  Done 6  Done 7  Done 8  Done 9  Done 10  Working on section 4 of 6.\n",
      "Done 1  Done 2  Done 3  Done 4  Done 5  Done 6  Done 7  Done 8  Done 9  Done 10  Working on section 5 of 6.\n",
      "Done 1  Done 2  Done 3  Done 4  Done 5  Done 6  Done 7  Done 8  Done 9  Done 10  Working on section 6 of 6.\n",
      "Done 1  Done 2  Done 3  Done 4  Done 5  Done 6  Done 7  Done 8  Done 9  Done 10  "
     ]
    }
   ],
   "source": [
    "article_sections_recommended_titles = list()\n",
    "for i, sec in enumerate(sections):\n",
    "    print(\"Working on section {0} of {1}.\".format(i+1, len(sections)))\n",
    "    article_sections_recommended_titles.append(get_recommend_articles(sec))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_recommend_articles(sections[1]).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "C Sharp (programming language) 5\n",
      "C (programming language) 4\n",
      "C++ 4\n",
      "List of superhuman features and abilities in fiction 3\n",
      "Beechcraft C-12 Huron 3\n",
      "\n",
      "\n",
      "\n",
      "C Sharp (programming language) 5\n",
      "C standard library 4\n",
      "C++ 4\n",
      "ANSI C 4\n",
      "C++ Standard Library 4\n",
      "\n",
      "\n",
      "\n",
      "Function object 4\n",
      "Virtual image 3\n",
      "First-class citizen 3\n",
      "C mathematical functions 3\n",
      "Network function virtualization 3\n",
      "\n",
      "\n",
      "\n",
      "C++ Standard Library 6\n",
      "C standard library 4\n",
      "C++ 4\n",
      "Standard Template Library 4\n",
      "C mathematical functions 3\n",
      "\n",
      "\n",
      "\n",
      "C data types 3\n",
      "C mathematical functions 3\n",
      "Citroën Type C 3\n",
      "C standard library 3\n",
      "C++ 3\n",
      "\n",
      "\n",
      "\n",
      "C++ 3\n",
      "GNU C Library 3\n",
      "Lockheed C-141 Starlifter 2\n",
      "C data types 2\n",
      "Criticism of C++ 2\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(article_sections_recommended_titles))\n",
    "for art in article_sections_recommended_titles:\n",
    "    for title, count in art.most_common(5):\n",
    "        print(title, count)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_text = open(\"test.txt\").read().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ï»¿1 00:00:00,470 --> 00:00:03,210 But first, I want to introduce you to another idea.  2 00:00:03,210 --> 00:00:05,000 Itâ€™s the idea of 1 x 1 convolutions.  3 00:00:06,030 --> 00:00:10,560 You might wonder, why one would ever want to use 1 x 1 convolutions?  4 00:00:10,560 --> 00:00:13,900 Theyâ€™re not really looking at a patch of the image just that one pixel.  5 00:00:15,120 --> 00:00:19,860 Look at the classic convolution setting, itâ€™s basically a small classifier for  6 00:00:19,860 --> 00:00:23,430 a patch of the image but it's only a linear classifier.  7 00:00:24,880 --> 00:00:29,160 But if you add a 1 x 1 convolution in the middle, suddenly you have a mini  8 00:00:29,160 --> 00:00:33,920 neural network running over the patch instead of a linear classifier.  9 00:00:33,920 --> 00:00:38,832 Interspersing your convolutions with 1 x 1 convolutions is a very inexpensive  10 00:00:38,832 --> 00:00:42,297 way to make your models deeper and have more parameters,  11 00:00:42,297 --> 00:00:45,273 without completely changing their structure.  12 00:00:45,273 --> 00:00:48,362 They're also very cheap because if you go through their math,  13 00:00:48,362 --> 00:00:50,462 they're not really convolutions at all,  14 00:00:50,462 --> 00:00:54,563 they're really just matrix multiplies and they have relatively few parameters.  15 00:00:54,563 --> 00:00:57,190 I mentioned all of these, average pulling and  16 00:00:57,190 --> 00:01:01,500 1 x 1 convolutions because I want to talk about the general strategy that has  17 00:01:01,500 --> 00:01:05,337 been very successful at creating cognates that are both smaller and  18 00:01:05,337 --> 00:01:09,070 better than cognates that simply use a pyramid of convolutions.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-51a51e353ce6>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-51a51e353ce6>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    think about how improve section theme detection\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "get_recommend_articles(sample_text).most_common()\n",
    "think about how improve section theme detection\n",
    "maybe keep register of wikipedia documents\n",
    "extract diferent size ngrams to search for articles\n",
    "use search engines apis to search on wikipedia site\n",
    "use sections hierarquical flows (maybe because we need actually is books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
