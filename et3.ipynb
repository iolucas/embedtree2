{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Swift 2.0 and Sprite Kit Basics for Game Developers\n"
     ]
    }
   ],
   "source": [
    "#Load data and test data\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "udemy_folder = \"udemy_data\"\n",
    "udemydata_folders = [\n",
    "    'dev', \n",
    "    'acad',\n",
    "    \"it\",\n",
    "    'mkt',\n",
    "    'op'\n",
    "]\n",
    "\n",
    "#Create data paths\n",
    "data_paths = list()\n",
    "for subf in udemydata_folders:\n",
    "    folder_path = udemy_folder + \"/\" + subf\n",
    "    for f in os.listdir(folder_path):\n",
    "        data_paths.append(folder_path + \"/\" + f)\n",
    "        \n",
    "        \n",
    "udemy_data = list()        \n",
    "for i, d_path in enumerate(data_paths):\n",
    "    course_data = json.load(open(d_path))\n",
    "    udemy_data.append(course_data)\n",
    "    #print(\"Done {}/{}\".format(i+1, len(data_paths)))\n",
    "\n",
    "print(\"Done\")\n",
    "print(udemy_data[1000][\"t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Topics that are meaningless and too general such introduction and conclusion\n",
    "#The generality also couples themes that have not in common\n",
    "excluded_topics = [\n",
    "    'introduction',\n",
    "    'conclusion',\n",
    "    'title to be edited here',\n",
    "    'summary',\n",
    "    'getting started',\n",
    "    'course introduction',\n",
    "    'introducción',\n",
    "    'section introduction',\n",
    "    'intro',\n",
    "    'prerequisites',\n",
    "    'introdução',\n",
    "    'section summary',\n",
    "    'bonus',\n",
    "    'learning objectives',\n",
    "    'course overview',\n",
    "    'overview',\n",
    "    'welcome',\n",
    "    'course summary',\n",
    "    'section recap',\n",
    "    'section introduction',\n",
    "    'introduction to the course',\n",
    "    'project intro',\n",
    "    'bonus lecture',\n",
    "    'section intro',\n",
    "    'comments',\n",
    "    'exercise',\n",
    "    'bonus material',\n",
    "    '01. introduction',\n",
    "    'section overview',\n",
    "    'challenge',\n",
    "    'resources',\n",
    "    'outro',\n",
    "    'course conclusion',\n",
    "    'welcome!',\n",
    "    'section review',\n",
    "    'bonus section',\n",
    "    'introduction to the course',\n",
    "    'important - download these first - working files',\n",
    "    'check your understanding',\n",
    "    'quiz',\n",
    "    'thank you',\n",
    "    'thank you!',\n",
    "    'introduction and objectives',\n",
    "    'einleitung',\n",
    "    'section conclusion',\n",
    "    'congratulations!',\n",
    "    'start here',\n",
    "    'final thoughts',\n",
    "    'final words',\n",
    "    'next steps',\n",
    "    'conclusions',\n",
    "    'knowledge check',\n",
    "    'chapter 2 & 3 quiz',\n",
    "    'chapter 1 quiz',\n",
    "    'chapter 1 & 2 quiz',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create topics counter and topics pre requisites counter\n",
    "We must create two collections: **topics_counter** and **topics_prereqs**.\n",
    "\n",
    "**topics_counter** will be a counter to accumulate frequency of all the different topics that occurs in the dataset.\n",
    "\n",
    "**topics_prereqs** will be a dict that for a given topic, computes the frequency of its prerequisites (that are also topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494851\n",
      "494851\n"
     ]
    }
   ],
   "source": [
    "#Get topics prereqs and function to get prereqs\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#Topics pre requisites will be every topic that is cited before it \n",
    "#As we add more and more learn references, for each topic we sum up the contents\n",
    "#So the more requested topics tend to have higher sum (score)\n",
    "\n",
    "#We may take diferent approachs for diferent subsections levels, \n",
    "#like place as prereq every subsection level above the current target \n",
    "#or only the the subsections of the same level\n",
    "\n",
    "topics_counter = Counter()\n",
    "topics_prereqs = defaultdict(Counter)\n",
    "\n",
    "for course in udemy_data: #Iterate thru every course\n",
    "    \n",
    "    #Lists to register the current course contents that already happend in subsecs degrees 1 and 2\n",
    "    course_subsec1_contents = list() \n",
    "    course_subsec2_contents = list()    \n",
    "    \n",
    "    for subsec1_content in course[\"c\"]: #Iterate thru content\n",
    "        subsec1_title = subsec1_content[\"t\"].lower()\n",
    "        \n",
    "        if subsec1_title in excluded_topics:\n",
    "            continue\n",
    "        \n",
    "        topics_prereqs[subsec1_title].update(course_subsec1_contents) #For the current, append everything before\n",
    "        course_subsec1_contents.append(subsec1_title)\n",
    "        \n",
    "        topics_counter[subsec1_title] += 1 #Update topics_counter\n",
    "        #continue\n",
    "        \n",
    "        #Analog to sub sec 2\n",
    "        for subsec2_content in subsec1_content[\"c\"]:\n",
    "            subsec2_title = subsec2_content[\"t\"].lower()\n",
    "            \n",
    "            if subsec2_title in excluded_topics:\n",
    "                continue\n",
    "            \n",
    "            #topics_prereqs[subsec2_title].update(course_subsec1_contents)\n",
    "            topics_prereqs[subsec2_title].update(course_subsec2_contents)\n",
    "            course_subsec2_contents.append(subsec2_title)\n",
    "            \n",
    "            topics_counter[subsec2_title] += 1 #Update topics_counter\n",
    "        \n",
    "print(len(topics_counter))\n",
    "print(len(topics_prereqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to return the calculated pre requisites of some topic\n",
    "def get_topic_prereqs(topic):\n",
    "\n",
    "    #subtract occurrences of the target knowledge from the pre reqs candidates\n",
    "    #Must take care with low occurrences contents titles because they will tend to stay at 1, since they happen low times and rarely are cited by other things\n",
    "\n",
    "    prereqs_diff = Counter()\n",
    "    for prereq in topics_prereqs[topic].keys():\n",
    "\n",
    "        #Use get function to avoid modify the prereqs counter dict\n",
    "        prereq_count = topics_prereqs[topic].get(prereq, 0) #Get the degree of need of the prereq in the target\n",
    "        topic_count = topics_prereqs[prereq].get(topic, 0) #Get the degree of need of the target in the prereq, return 0 if not found\n",
    "\n",
    "        prereqs_diff[prereq] = prereq_count - topic_count\n",
    "        \n",
    "    return prereqs_diff.most_common()\n",
    "\n",
    "#get_topic_prereqs(\"javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Search function to find topics\n",
    "def search_topics(term):\n",
    "    term = term.lower()\n",
    "    #Return words that got the term, sorting by most used\n",
    "    results = [(topic, count) for topic, count in topics_counter.items() if term in topic]\n",
    "    \n",
    "    #return sorted results by count\n",
    "    return sorted(results, key=lambda a: a[1], reverse=True)\n",
    "\n",
    "#search_topics(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function the parent section of some content\n",
    "def find_content_parent_section(target_search):\n",
    "    search_result = list()\n",
    "    #target_search = \"positioning\"\n",
    "    for course in udemy_data:\n",
    "        course_title = course[\"t\"]\n",
    "        for c in course[\"c\"]:\n",
    "            c_title = c[\"t\"].lower()\n",
    "\n",
    "            if c_title == target_search:\n",
    "                search_result.append(course_title)\n",
    "\n",
    "            for c2 in c[\"c\"]:\n",
    "                c2_title = c2[\"t\"].lower()\n",
    "\n",
    "                if c2_title == target_search:\n",
    "                    search_result.append(c_title)\n",
    "                \n",
    "    return search_result\n",
    "\n",
    "#find_content_parent_section(\"javascript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create function to get the prereqs that occurs above a certain cut value\n",
    "def get_cutted_prereq(topic, cut_value=0):\n",
    "    #get prereqs for the topic\n",
    "    topic_prereqs = get_topic_prereqs(topic)\n",
    "    cutted_prereqs = [pr for pr, count in topic_prereqs if count > cut_value]\n",
    "    return cutted_prereqs\n",
    "\n",
    "#Create function to get the entire prereq flow recursively\n",
    "def get_prereq_flow_members(topic,cut_value, prereq_dict):\n",
    "    topic_prereqs = get_cutted_prereq(topic, cut_value)\n",
    "    prereq_dict[topic] = topic_prereqs\n",
    "    \n",
    "    for pr in topic_prereqs:\n",
    "        \n",
    "        if pr not in prereq_dict:\n",
    "            prereq_dict = get_prereq_flow_members(pr, cut_value, prereq_dict)\n",
    "        \n",
    "    return prereq_dict  \n",
    "\n",
    "#Function to get the entire prereq flow recursively,\n",
    "#Limiting content to the target content cutted prereq\n",
    "#So for child content we place cut_value to reasonable value, like 1\n",
    "def get_filtered_prereq_flow_members(topic, cut_value, filter_set=None, prereq_dict=None):\n",
    "    if prereq_dict == None:\n",
    "        prereq_dict = dict()\n",
    "        \n",
    "    topic_cutted_prereqs = get_cutted_prereq(topic, cut_value)\n",
    "    \n",
    "    #If no filter set has been specified, create one with the result\n",
    "    if filter_set == None:\n",
    "        filter_set = set(topic_cutted_prereqs)\n",
    "        \n",
    "    #Filter the result with the filter set\n",
    "    filtered_topic_prereqs = list(filter_set.intersection(set(topic_cutted_prereqs)))\n",
    "    #filtered_topic_prereqs = topic_cutted_prereqs\n",
    "    \n",
    "    #Place data to dict\n",
    "    prereq_dict[topic] = filtered_topic_prereqs\n",
    "    \n",
    "    for pr in filtered_topic_prereqs:\n",
    "        if pr not in prereq_dict:\n",
    "            prereq_dict = get_filtered_prereq_flow_members(pr, 1, filter_set, prereq_dict)\n",
    "    \n",
    "    return prereq_dict\n",
    "\n",
    "#get_filtered_prereq_flow_members(\"binary trees\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create function to create the spanning tree of prereq flows, that is the optimal \"learning path\"\n",
    "#With optimal we mean learn only what is necessary and on the time that is necessary\n",
    "import networkx as nx\n",
    "\n",
    "#Deprecated due to not use\n",
    "def DEPRECATED_get_topic_prereq_flow_graph(topic, cut_value=0):\n",
    "    #prereq_flow_members = get_prereq_flow_members(topic, cut_value, dict())\n",
    "    prereq_flow_members = get_filtered_prereq_flow_members(topic, cut_value)\n",
    "\n",
    "    #MUST CHECK IF THE PROCESS TIL HERE ALREADY AVOID CREATION OF CIRCLES\n",
    "    flow_graph = nx.DiGraph()\n",
    "    \n",
    "    for source, targets in prereq_flow_members.items():\n",
    "        for target in targets:\n",
    "            flow_graph.add_edge(source, target)\n",
    "            \n",
    "    return flow_graph  \n",
    "    \n",
    "    \n",
    "def DEPRECATED_get_optimal_topic_prereq_flow_graph(topic, cut_value=0):\n",
    "    #we treat every branch as diferent path to learn something,\n",
    "    #But if we find some of the branchs inside other branch, we may remove it\n",
    "    #In order to have a more clean visualization\n",
    "    \n",
    "    #The data origin naturally avoid parallel needing (MUST CHECK THIS)\n",
    "    #MUST FIND A WAY TO DETECT TWO PARALLEL NEEDING AND TWO PATHS TO THE SAME THING\n",
    "    #this is parallel needing since we take everything before and each thing we add as a need\n",
    "    \n",
    "    #Deprecated: we won't use arrow like stuff, but levels columns instead\n",
    "    \n",
    "    flow_graph = get_topic_prereq_flow_graph(topic, cut_value)\n",
    "    optimal_flow_graph = nx.Edmonds(flow_graph)\n",
    "    return optimal_flow_graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to return each node level for arrangement\n",
    "Node levels should be low for the more basic stuff and high for complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_levels_by_pagerank(graph, round_decimals=2):\n",
    "    graph_pr = nx.pagerank(graph)\n",
    "    graph_pr_rounded = dict([(node, round(level, round_decimals)) for node, level in graph_pr.items()])\n",
    "    return graph_pr_rounded\n",
    "\n",
    "def get_levels_by_depth_first(graph, source):\n",
    "    node_levels = defaultdict(int)\n",
    "    node_levels[source] = 0\n",
    "    for source, target in nx.dfs_edges(graph, source):\n",
    "        node_levels[target] = min(node_levels[target], node_levels[source] - 1)\n",
    "    return node_levels\n",
    "\n",
    "\n",
    "#We may improve this doing by hand returning in case the path length already passed the previous\n",
    "def get_levels_by_paths_length(graph, source_node, cutoff=None):\n",
    "    node_levels = defaultdict(int)\n",
    "    node_levels[source_node] = 0\n",
    "    \n",
    "    for target_node in graph.nodes():\n",
    "        for path in nx.all_simple_paths(graph, source_node, target_node, cutoff):\n",
    "            node_levels[target_node] = min(len(path)*-1, node_levels[target_node])\n",
    "        \n",
    "    #print(node_levels)\n",
    "    \n",
    "    return node_levels\n",
    "    \n",
    "#Get node levels by going thru a depth first but including already passed node\n",
    "def get_levels_by_inclusive_depth_first(graph, source):\n",
    "    node_levels = dict()\n",
    "    visited_list = list()\n",
    "    recursion_depth = [0]\n",
    "    \n",
    "    def get_node_level(node):\n",
    "        if node in node_levels:\n",
    "            return node_levels[node]\n",
    "        \n",
    "        recursion_depth[0] += 1\n",
    "        visited_list.append(node)\n",
    "        \n",
    "        node_level = 0\n",
    "        for target in graph[node].keys():\n",
    "            if target in visited_list:\n",
    "                continue\n",
    "            \n",
    "            target_level = get_node_level(target)\n",
    "            node_level = max(target_level+1, node_level)\n",
    "        \n",
    "        node_levels[node] = node_level\n",
    "        \n",
    "        visited_list.remove(node)\n",
    "        recursion_depth[0] -= 1\n",
    "        return node_level\n",
    "    \n",
    "    try:\n",
    "        get_node_level(source)\n",
    "    except RecursionError:\n",
    "        print(\"ResursionError:\\nRecursion depth: \", recursion_depth[0])\n",
    "        assert False\n",
    "\n",
    "    \n",
    "    return node_levels\n",
    "\n",
    "    #def transverse_and_set_level(source_node):\n",
    "        #node_level = 0\n",
    "        #for target_node in graph[source_node].keys():\n",
    "            #target_level \n",
    "    \n",
    "    #def set_node_level(node, level):\n",
    "        #global recurse_depth\n",
    "        #global max_depth\n",
    "        #node_levels[node] = max(node_levels[node], level)\n",
    "        #for target in flow_graph[node].keys():\n",
    "            #recurse_depth += 1\n",
    "            #max_depth = max(max_depth, recurse_depth)\n",
    "            #set_node_level(target, level+1)\n",
    "            #recurse_depth -=1\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_levels_by_starting_nodes_propagation():\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_data(headers, data_list):\n",
    "    \"\"\"Function to print data tabulated \"\"\"\n",
    "    return print(tabulate(data_list, headers=headers))\n",
    "\n",
    "#cut value is relative for each topic, so we cant propagate it down the prerequisite chain\n",
    "#This way we will limit the prereq chain to only the members of the main chain \n",
    "#and place their levels according to the pre requisites of the filtered pre requisites\n",
    "\n",
    "\n",
    "def print_optimal_topic_prereq_flow(topic, cut_value=0):\n",
    "    #prereq_flow_members = get_prereq_flow_members(topic, cut_value, dict())\n",
    "    prereq_flow_members =  get_filtered_prereq_flow_members(topic, cut_value)\n",
    "\n",
    "\n",
    "    #1. Construct graph\n",
    "    flow_graph = nx.DiGraph()\n",
    "    flow_graph.add_node(topic) #Ensure main node is present in the graph\n",
    "    for source, targets in prereq_flow_members.items():\n",
    "        for target in targets:\n",
    "            flow_graph.add_edge(source, target)\n",
    "\n",
    "    \n",
    "    #2. Get node levels\n",
    "    #node_levels = get_levels_by_depth_first(flow_graph, topic)\n",
    "    #node_levels = get_levels_by_pagerank(flow_graph, 2)\n",
    "    #node_levels = get_levels_by_paths_length(flow_graph, topic, None)\n",
    "    node_levels = get_levels_by_inclusive_depth_first(flow_graph, topic)\n",
    "    \n",
    "        \n",
    "    #3. Create print table    \n",
    "    print_tables = defaultdict(list)\n",
    "    for node, level in node_levels.items():\n",
    "        print_tables[level].append(node)\n",
    "        \n",
    "    sorted_print_table = sorted(print_tables.items(), key=lambda a: int(a[0]))\n",
    "    print_table_list = [cont for _, cont in sorted_print_table]\n",
    "    \n",
    "    for sec in print_table_list:\n",
    "        print(\"--------------------\")\n",
    "        for topic in sec:\n",
    "            print(topic)\n",
    "    \n",
    "    return flow_graph\n",
    "    \n",
    "    print_table_list_dict = dict(enumerate(print_table_list))\n",
    "        \n",
    "\n",
    "        \n",
    "    #4. Print topics prereq flow\n",
    "    tabulate_print = tabulate(print_table_list_dict, headers=\"keys\")\n",
    "    #print(len(tabulate_print))\n",
    "    print(tabulate_print)\n",
    "        \n",
    "    return flow_graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xml', 13),\n",
       " ('introduction to xml', 7),\n",
       " ('xml sitemaps', 6),\n",
       " ('parsing xml', 5),\n",
       " ('xml intro', 4),\n",
       " ('xml serialization', 4),\n",
       " ('xml basics', 4),\n",
       " ('testng.xml code', 3),\n",
       " ('javascript xmlhttprequest & web apis', 3),\n",
       " ('working with xml', 3),\n",
       " ('why xml?', 3),\n",
       " ('build.xml code', 3),\n",
       " ('xml attributes', 3),\n",
       " ('including and excluding the testcases from execution with testng xml file',\n",
       "  3),\n",
       " ('xml sitemap', 3),\n",
       " ('json vs xml', 3),\n",
       " ('build phases & pom.xml', 2),\n",
       " ('becoming familiar with the xml layouts', 2),\n",
       " ('google xml sitemaps', 2),\n",
       " ('pom xml file code', 2),\n",
       " ('maven pom.xml file explanation', 2),\n",
       " ('linear layout designing of questions.xml of android interview app : part-18',\n",
       "  2),\n",
       " ('adding live web content with xml and json', 2),\n",
       " ('maven configuration and pom.xml', 2),\n",
       " ('xml y onclick', 2),\n",
       " ('xml schema hands on', 2),\n",
       " ('adding graphics to \" questions.xml \" for multiple screen size & density',\n",
       "  2),\n",
       " ('understanding build.xml file', 2),\n",
       " ('criando xml resource color style para cabecalhos e rodapes', 2),\n",
       " ('manipulando xml', 2),\n",
       " ('날씨앱 - xml parsing', 2),\n",
       " ('xml and web services', 2),\n",
       " ('introduction to xml publisher', 2),\n",
       " ('02. adding a new layout and talking about xml.mov', 2),\n",
       " ('creating menus by xml code', 2),\n",
       " ('xml naming', 2),\n",
       " ('xml schema documents - demo', 2),\n",
       " ('processing xml', 2),\n",
       " ('xml (construcción de niveles)', 2),\n",
       " ('xml quiz', 2),\n",
       " ('1045 - parsing xml', 2),\n",
       " ('introduction to xml and json data formats', 2),\n",
       " ('ant build.xml file download', 2),\n",
       " ('generating java classes from xml schema', 2),\n",
       " ('o que é xml? e porque usamos?', 2),\n",
       " ('what is xml?', 2),\n",
       " ('entenda o arquivo dimens.xml', 2),\n",
       " ('ajax and xml', 2),\n",
       " ('androidmanifest.xml', 2),\n",
       " ('xml e dom con php', 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#topics_counter.most_common(50)\n",
    "search_topics(\"xml\")[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keep on it!!\n",
    "## maybe use rnn to detect the data we should display as prereq\n",
    "## how will differ two things that have the same name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "html\n",
      "--------------------\n",
      "css\n",
      "--------------------\n",
      "javascript\n"
     ]
    }
   ],
   "source": [
    "flow_graph = print_optimal_topic_prereq_flow('javascript',10)\n",
    "#print(flow_graph.edges())\n",
    "#flow_graph = print_optimal_topic_prereq_flow(\"jquery\", 7)\n",
    "#flow_graph.edges()\n",
    "#keep working on this\n",
    "#maybe create interface to search and check\n",
    "#get more data to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
