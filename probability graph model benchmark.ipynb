{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Probability Graph Model Benchmark\n",
    "Whithin this notebook we will compare technics to evaluate a probability graph model for a given seed article through out its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wikipydia import dict_storage, wikidb, parse, wikisyn\n",
    "WikisynDict = wikisyn.WikisynDict\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "from ThreadPool import ThreadPool\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_db = wikidb.WikiDb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_data(headers, data_list):\n",
    "    \"\"\"Function to print data tabulated \"\"\"\n",
    "    return print(tabulate(data_list, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_percent_reported = None\n",
    "def download_progress_hook(count, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to download bunch of wikipedia pages at once if they are not present\n",
    "check_and_download__done = 0\n",
    "def check_and_download(pages):\n",
    "    global check_and_download__done\n",
    "    check_and_download__done = 0\n",
    "    \n",
    "    n_tasks = len(pages)\n",
    "    \n",
    "    print(\"Tasks to go: \" + str(n_tasks))\n",
    "    \n",
    "    # Function to be executed in a thread\n",
    "    def download_stuff(page):\n",
    "        global check_and_download__done\n",
    "        #try:\n",
    "        wiki_db.get_article_by_href(page)\n",
    "        #except:\n",
    "            #print(\"Failed to get page \" + page + \". Timed out.\")\n",
    "        #finally:\n",
    "        check_and_download__done += 1\n",
    "        download_progress_hook(check_and_download__done, n_tasks)\n",
    "            #percentage = round(check_and_download__done * 100 / n_tasks)\n",
    "            #if percentage % 5 == 0:\n",
    "                #print(\"{0}%.....\".format(percentage), end=\"\")\n",
    "            #print(\"Done \" + str(check_and_download__done) + \"/\" + str(n_tasks))\n",
    "\n",
    "    # Instantiate a thread pool with 5 worker threads\n",
    "    pool = ThreadPool(50)\n",
    "\n",
    "    pool.map(download_stuff, pages)\n",
    "    pool.wait_completion()\n",
    "    \n",
    "    print(\"\\nFinishing downloading. Done tasks: \" + str(check_and_download__done) + \"/\" + str(n_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_href_text_count(href):\n",
    "    synDict = WikisynDict()\n",
    "    \n",
    "    wikiart, _ = wiki_db.get_article_by_href(href)\n",
    "    links = wikiart.links()\n",
    "    page_text = wikiart.text()\n",
    "    \n",
    "    #Populate syndict\n",
    "    for href, text in links:\n",
    "        synDict.add_link(href, text)\n",
    "        \n",
    "    #Get matches\n",
    "    links_score = Counter()\n",
    "        \n",
    "    for link_href in synDict.keys():\n",
    "        for l_text, l_score in synDict[link_href]:\n",
    "            matches = re.findall('[^a-zA-Z0-9_]' + re.escape(l_text) + '[^a-zA-Z0-9_]', page_text, re.IGNORECASE)\n",
    "            matches_score = len(matches) * l_score\n",
    "            links_score[link_href] += matches_score\n",
    "\n",
    "    scores_sum = sum(links_score.values())\n",
    "            \n",
    "    #norm_links_scores = list(map(lambda a: (a[0],synDict[a[0]], a[1]/scores_sum), links_score.most_common()))\n",
    "    norm_links_scores = list(map(lambda a: (a[0], a[1]/scores_sum), links_score.most_common()))\n",
    "    #norm_links_scores = list(map(lambda a: (a[0],list(map(lambda b: b[0], synDict[a[0]])), a[1]/scores_sum), links_score.most_common()))\n",
    "    \n",
    "    return norm_links_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_node_edges_scores(page_href, top=-1):\n",
    "    \"\"\"Function to get node edges to be placed in the graph. \"\"\"\n",
    "    \n",
    "    #edges = dict()\n",
    "    edges = Counter()\n",
    "    \n",
    "    #wikiart = get_wiki_article_by_href(page_href)\n",
    "    wikiart, _ = wiki_db.get_article_by_href(page_href)\n",
    "    \n",
    "    #Get main page data\n",
    "    page_id = wikiart.page_id()\n",
    "    page_title = wikiart.title()\n",
    "    #page_links = get_links_score(page_href).items()\n",
    "    page_links = get_href_text_count(page_href)\n",
    "    \n",
    "    if top > -1:\n",
    "        sorted_page_links = sorted(page_links, key=lambda a: a[1], reverse=True)\n",
    "        page_links = sorted_page_links[:top]\n",
    "        #print(page_links)\n",
    "        \n",
    "    #Ensure everything has been downloaded first    \n",
    "    hrefsToDownload = [link_href for link_href, _ in page_links]        \n",
    "    check_and_download(hrefsToDownload)\n",
    "    \n",
    "    #Get links score sum\n",
    "    score_sum = 0\n",
    "    for _, score in page_links:\n",
    "        score_sum += score\n",
    "    \n",
    "    for i, (link_href, score) in enumerate(page_links):\n",
    "        \n",
    "        #link_art = get_wiki_article_by_href(link_href)\n",
    "        link_art, _ = wiki_db.get_article_by_href(link_href)\n",
    "        \n",
    "        link_id = link_art.page_id()\n",
    "        link_title = link_art.title()\n",
    "        \n",
    "        #If there is already a title already place, sum the scores\n",
    "        edges[(page_title, link_title)] += score / score_sum\n",
    "        \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks to go: 20\n",
      "5%10%15%20%25%35%40%30%60%50%65%70%45%55%75%85%90%95%100%\n",
      "Finishing downloading. Done tasks: 20/20\n",
      "Link                                                  Score\n",
      "------------------------------------------------  ---------\n",
      "('TensorFlow', 'Open-source model')               0.226415\n",
      "('TensorFlow', 'Artificial neural network')       0.188679\n",
      "('TensorFlow', 'Machine learning')                0.113208\n",
      "('TensorFlow', 'Deep learning')                   0.0566038\n",
      "('TensorFlow', 'Google Brain')                    0.0566038\n",
      "('TensorFlow', 'C++')                             0.0377358\n",
      "('TensorFlow', 'RankBrain')                       0.0377358\n",
      "('TensorFlow', 'Supervised learning')             0.0377358\n",
      "('TensorFlow', 'Jeff Dean (computer scientist)')  0.0377358\n",
      "('TensorFlow', 'Python (programming language)')   0.0377358\n",
      "('TensorFlow', 'Tensor processing unit')          0.0377358\n",
      "('TensorFlow', 'MacOS')                           0.0188679\n",
      "('TensorFlow', 'Neuroph')                         0.0188679\n",
      "('TensorFlow', 'Graphics processing unit')        0.0188679\n",
      "('TensorFlow', 'MXNet')                           0.0188679\n",
      "('TensorFlow', 'Torch (machine learning)')        0.0188679\n",
      "('TensorFlow', 'Directed graph')                  0.0188679\n",
      "('TensorFlow', 'GitHub')                          0.0188679\n"
     ]
    }
   ],
   "source": [
    "edges_score = get_node_edges_scores(\"TensorFlow\", 20)\n",
    "wiki_db.save()\n",
    "print_data([\"Link\", \"Score\"], sorted(edges_score.items(), key=lambda a: a[1], reverse=True))\n",
    "\n",
    "READ hidden markov models\n",
    "watch reinforcement\n",
    "implement the rest of the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
