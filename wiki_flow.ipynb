{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the wikipedia article\n",
    "from wikipydia import dict_storage, wikidb, parse, wikisyn\n",
    "from wiki_sections import get_article_obj\n",
    "wiki_db = wikidb.WikiDb()\n",
    "\n",
    "def get_article(href):\n",
    "    \n",
    "    article, _ = wiki_db.get_article_by_href(href)\n",
    "    wiki_db.save()\n",
    "    \n",
    "    return get_article_obj(article.title(), article.html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the keywords from a piece of text\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union(set(punctuation)).union(set([\"''\", \"``\"]))\n",
    "\n",
    "def get_text_keywords(text, n=10):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    token_counter = Counter()\n",
    "    \n",
    "    for word in tokens:\n",
    "        if len(word) <= 1:\n",
    "            continue\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "    \n",
    "        token_counter[word] += 1\n",
    "    \n",
    "    #return token_counter.most_common(n)\n",
    "    \n",
    "    return [w for w, count in token_counter.most_common(n)]\n",
    "\n",
    "#get_text_keywords(\"Linear algebra is the branch of mathematics concerning vector spaces and linear mappings between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to search wikipedia articles\n",
    "\n",
    "import requests\n",
    "\n",
    "def search_wikipedia(term, lang=\"en\", timeout=60):\n",
    "    \"\"\"Function search articles on wikipedia.\"\"\"\n",
    "    #import exceptions\n",
    "\n",
    "    #https://en.wikipedia.org/w/api.php?action=query&utf8&list=search&srsearch=neural\n",
    "    #https://www.mediawiki.org/wiki/API:Search\n",
    "\n",
    "    # https://en.wikipedia.org/w/api.php?action=parse&redirects&page=fluid_mechanics\n",
    "\n",
    "    req_params = [\n",
    "        'action=query',\n",
    "        'utf8',\n",
    "        'list=search',\n",
    "        'format=json',\n",
    "        'srwhat=text',\n",
    "        'srprop',\n",
    "        'srlimit=500',\n",
    "        'srsearch=' + term\n",
    "    ]\n",
    "\n",
    "    wikipedia_api_url = \"https://\" + lang + \".wikipedia.org/w/api.php?\" + \"&\".join(req_params)\n",
    "\n",
    "    page_data = requests.get(wikipedia_api_url, timeout=timeout).json()\n",
    "\n",
    "    results = [result['title'] for result in page_data['query']['search']]\n",
    "    \n",
    "    return results\n",
    "\n",
    "#search_wikipedia(\"neural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying plcontinue: None\n",
      "\n",
      "Artificial neural network 344\n",
      "Pattern recognition 201\n",
      "Recurrent neural network 187\n",
      "Machine learning 314\n",
      "Linear algebra 240\n",
      "Autoencoder 94\n",
      "MQTT 76\n",
      "Hierarchical temporal memory 54\n",
      "sofuasduasjd 0\n",
      "Deep learning 397\n",
      "Feature learning 115\n",
      "Sepp Hochreiter 125\n",
      "Convolutional neural network 175\n"
     ]
    }
   ],
   "source": [
    "DictStorage = dict_storage.DictStorage\n",
    "wiki_links = DictStorage(\"wiki_links\")\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_wikipedia_links(pages):\n",
    "    \n",
    "    pages_to_download = list()\n",
    "    \n",
    "    for page in pages:\n",
    "        if page.lower() not in wiki_links:\n",
    "            pages_to_download.append(page)\n",
    "    \n",
    "    #If there is pages not locally present, download and save data\n",
    "    if len(pages_to_download) > 0:\n",
    "        #Query all the pages not present\n",
    "        downloaded_links = query_wikipedia_links(pages_to_download)\n",
    "        for t, l in downloaded_links.items():\n",
    "            wiki_links[t.lower()] = l\n",
    "        wiki_links.save()\n",
    "    \n",
    "    links_to_return = dict()\n",
    "    for page in pages:\n",
    "        links_to_return[page] = wiki_links[page.lower()]\n",
    "        \n",
    "    return links_to_return\n",
    "    \n",
    "def query_wikipedia_links(pages, lang=\"en\", timeout=60):\n",
    "    \n",
    "    links = defaultdict(set)\n",
    "    plcontinue = None\n",
    "    \n",
    "    while True:\n",
    "        print(\"Querying plcontinue: \" + str(plcontinue))\n",
    "        \n",
    "        partial_links, plcontinue = partial_query_wikipedia_links(pages, plcontinue)\n",
    "        \n",
    "        for title, link_set in partial_links.items():\n",
    "            links[title] = links[title].union(link_set)\n",
    "\n",
    "        if not plcontinue:\n",
    "            break;\n",
    "            \n",
    "    return links\n",
    "        \n",
    "\n",
    "def partial_query_wikipedia_links(pages, plcontinue=None, lang=\"en\", timeout=60):\n",
    "    #https://en.wikipedia.org/w/api.php?action=query&titles=MQTT&prop=links&pllimit=500&pltitles=Adafruit|Internet%20layer\n",
    "    \n",
    "    req_params = [\n",
    "        'action=query',\n",
    "        'utf8',\n",
    "        'prop=links',\n",
    "        'plnamespace=0',\n",
    "        'format=json',\n",
    "        'pllimit=500',\n",
    "        'titles=' + \"|\".join(pages)\n",
    "    ]\n",
    "    \n",
    "    if plcontinue:\n",
    "        req_params.append(\"plcontinue=\" + plcontinue)\n",
    "\n",
    "    wikipedia_api_url = \"https://\" + lang + \".wikipedia.org/w/api.php?\" + \"&\".join(req_params)\n",
    "\n",
    "    page_data = requests.get(wikipedia_api_url, timeout=timeout).json()\n",
    "\n",
    "    \n",
    "    #If there is a continue page\n",
    "    if \"continue\" in page_data:\n",
    "        plcontinue_param = page_data['continue']['plcontinue']\n",
    "    else:\n",
    "        plcontinue_param = None\n",
    "        \n",
    "    links = dict()\n",
    "    \n",
    "    for pageid, data in page_data['query']['pages'].items():\n",
    "        page_title = data[\"title\"]\n",
    "        links[page_title] = set()\n",
    "        if \"links\" not in data:\n",
    "            continue\n",
    "        for l in data['links']:\n",
    "            links[page_title].add(l['title'])\n",
    "            \n",
    "    return links, plcontinue_param\n",
    "        \n",
    "    \n",
    "def test_query_wikilinks1():\n",
    "    \n",
    "    test_query_links = get_wikipedia_links(['Deep learning',\n",
    "      'Feature learning',\n",
    "      'Artificial neural network',\n",
    "      'Convolutional neural network',\n",
    "      'Recurrent neural network',\n",
    "      'Machine learning',\n",
    "      'Autoencoder',\n",
    "      'Hierarchical temporal memory',\n",
    "      'Pattern recognition',\n",
    "      'Sepp Hochreiter',\n",
    "      'sofuasduasjd',\n",
    "      \"MQTT\",\n",
    "      \"Linear algebra\"])\n",
    "\n",
    "    print(\"\")\n",
    "    for page, links in test_query_links.items():\n",
    "        print(page, len(links))\n",
    "        \n",
    "def test_query_wikilinks2():\n",
    "    print(wiki_links.items())\n",
    "    \n",
    "#test_query_wikilinks1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sections_text(href):\n",
    "    sections_text = list()\n",
    "    for sec in get_article(href):\n",
    "        sections_text.append(str(sec).replace(\"\\n\", \" \"))\n",
    "    return sections_text\n",
    "\n",
    "#len(get_sections_text(\"MQTT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Deep learning',\n",
       "  'Feature learning',\n",
       "  'Artificial neural network',\n",
       "  'Convolutional neural network',\n",
       "  'Recurrent neural network',\n",
       "  'Machine learning',\n",
       "  'Autoencoder',\n",
       "  'Hierarchical temporal memory',\n",
       "  'Pattern recognition',\n",
       "  'Sepp Hochreiter'],\n",
       " ['Feature learning',\n",
       "  'Deep learning',\n",
       "  'Machine learning',\n",
       "  'Autoencoder',\n",
       "  'Hierarchical temporal memory',\n",
       "  'Convolutional neural network',\n",
       "  'Pattern recognition',\n",
       "  'M-Theory (learning framework)',\n",
       "  'Sepp Hochreiter',\n",
       "  'Artificial intelligence'],\n",
       " ['Deep learning', 'Bayesian network'],\n",
       " ['Deep learning',\n",
       "  'Artificial neural network',\n",
       "  'Convolutional neural network',\n",
       "  'Recurrent neural network',\n",
       "  'Speech recognition',\n",
       "  'Vanishing gradient problem',\n",
       "  'Feature learning',\n",
       "  'Pattern recognition',\n",
       "  'Machine learning',\n",
       "  'Multilayer perceptron'],\n",
       " ['Artificial neural network',\n",
       "  'Deep learning',\n",
       "  'Convolutional neural network',\n",
       "  'Recurrent neural network',\n",
       "  'Convolutional Deep Belief Networks',\n",
       "  'Multilayer perceptron',\n",
       "  'Vanishing gradient problem',\n",
       "  'Instantaneously trained neural networks',\n",
       "  'Neural Designer',\n",
       "  'DeepDream'],\n",
       " ['Deep learning',\n",
       "  'Pattern recognition',\n",
       "  'Hidden Markov model',\n",
       "  'K-means clustering'],\n",
       " ['Deep learning', 'Nonlinear dimensionality reduction'],\n",
       " ['Deep learning',\n",
       "  'Speech recognition',\n",
       "  'Recurrent neural network',\n",
       "  'Artificial neural network',\n",
       "  'Vanishing gradient problem',\n",
       "  'Artificial intelligence'],\n",
       " ['Deep learning',\n",
       "  'Machine learning',\n",
       "  'Brain',\n",
       "  'Outline of the human brain',\n",
       "  'Nervous system network models',\n",
       "  'Lateralization of brain function',\n",
       "  'Artificial intelligence',\n",
       "  'Developmental robotics',\n",
       "  'Transfer of learning',\n",
       "  'Mind'],\n",
       " ['Deep learning',\n",
       "  'Facebook',\n",
       "  'Google',\n",
       "  'Oddworld Inhabitants',\n",
       "  'List of A Certain Magical Index characters',\n",
       "  'List of Mass Effect characters'],\n",
       " ['Deep learning',\n",
       "  'Artificial neural network',\n",
       "  'Artificial intelligence',\n",
       "  'Design thinking',\n",
       "  'Waldorf education',\n",
       "  'Music theory',\n",
       "  'AI winter',\n",
       "  'Positive psychology',\n",
       "  'Drama',\n",
       "  'Anglo-Saxons'],\n",
       " ['Deeplearning4j',\n",
       "  'Deep learning',\n",
       "  'List of numerical libraries',\n",
       "  'Convolutional neural network']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wiki_flow(href):\n",
    "    \n",
    "    #1. Get article sections text\n",
    "    sections_text = get_sections_text(href)\n",
    "    \n",
    "    #2. Get sections keywords\n",
    "    sections_keywords = list()\n",
    "    for sec_text in sections_text:\n",
    "        sections_keywords.append(get_text_keywords(sec_text))\n",
    "        \n",
    "    #3. Get articles sugestions for each section\n",
    "    sections_articles_candidates = list()\n",
    "    for sec_keywords in sections_keywords:\n",
    "        search_term = \" \".join(word for word in sec_keywords)\n",
    "        sections_articles_candidates.append(search_wikipedia(search_term)[:10])\n",
    "    \n",
    "    return sections_articles_candidates\n",
    "    \n",
    "    #2. Get the links from each article.\n",
    "    #3. Try to find an optimal flow following the links, starting from the last article, to the first.\n",
    "    \n",
    "    #sections_articles_candidates = list()\n",
    "    \n",
    "    #for sec_text in sections_text:\n",
    "        #Get snippet keywords\n",
    "        \n",
    "get_wiki_flow(\"Deep_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
