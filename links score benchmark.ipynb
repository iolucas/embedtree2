{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Important Links\n",
    "Algorithms to atribute article links importance scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "KEEP WORKING ON THIS FORMALIZATION AND COMPILATION OF TECHNICS TO COMPUTE IMPORTANT LINKS\n",
    "MAYBE EXPERIMENT WIKISYNS BASED ON BACKLINKS\n",
    "EXPERIMENT VARIATIONS OF WIKISYNS \n",
    "COMPUTE WORD IMPORTANCE BY ITS SPREAD IN THE ARTICLE (MAYBE TO THIS IN OTHER NOTEBOOK SINCE THIS ONE IS FOR LINKS)\n",
    "CREATE TREE WITH DICT SYMS\n",
    "\n",
    "YOU COULD HAVE A LINK STRUCTURE BASED ON UNSUPERVISIED MODELS AND NLP BASED ON SUPERVISED MODELS\n",
    "THE LINK STRUCTURE WOULD GIVE A BROAD PATH, MEANING THAT DEEP TRANSVERSAL WOULD MEAN SOMETHING YET, BUT NLP BASED WOULD ONLY BE LOCAL\n",
    "WE COULD USE WIKISYN TO FIND \"POSSIBLE LINKS\" THAT WERE NOT CREATED\n",
    "\n",
    "FOR SUPERVISED WE MAY USE:\n",
    "    NPM DATA\n",
    "    WORDNET (AND OTHERS) DICTIONARIES\n",
    "    SCHOOL AND COURSES CURRICULUMS \n",
    "\n",
    "LINKS PROBABILITIES ARE:\n",
    "    BIDIR PROBS\n",
    "    NUMBER OF LINKS PER PAGE PROB\n",
    "    WORD COUNT BASED PROB\n",
    "    \n",
    "    USE TRELLO TO ORGANIZE EVERTYHING\n",
    "    PLAN NEXT STEPS\n",
    "    MELHOR FEITO DO QUE PERFEITO\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wikipydia import dict_storage, wikidb, parse, wikisyn\n",
    "WikisynDict = wikisyn.WikisynDict\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_data(headers, data_list):\n",
    "    \"\"\"Function to print data tabulated \"\"\"\n",
    "    return print(tabulate(data_list, headers=headers))\n",
    "#print_data(['seedrank', 'pagerank', 'in_degree', 'ufauisfha', 'isfudnifsu'], [[1,2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_db = wikidb.WikiDb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_list(iterable):\n",
    "    for item in iterable:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arti, _ = wiki_db.get_article_by_href(\"Fluid\")\n",
    "#for link in arti.links():\n",
    "    #print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract Links only\n",
    "Get only the abstract links (first section) and set the same proportional score for all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_links(href):\n",
    "    wikiart, _ = wiki_db.get_article_by_href(href)\n",
    "    sections = parse._split_html_h2_sections(wikiart.html())\n",
    "    abstract_links_with_text = parse._filter_wiki_links(parse._extract_html_links(sections[0]))\n",
    "    \n",
    "    abstract_links = set([href for href, text in abstract_links_with_text])\n",
    "    \n",
    "    all_score = 1 / len(abstract_links)\n",
    "    \n",
    "    abstract_links_scores = [(href, all_score) for href in abstract_links]\n",
    "    \n",
    "    return abstract_links_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstract_links = get_abstract_links(\"Computer\")\n",
    "#print(len(abstract_links))\n",
    "#print_list(abstract_links[:20])\n",
    "#print_data(['Article', 'Score'], abstract_links[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple link occurrence count\n",
    "Just count the number of times some link occurs and score with it. \n",
    "\n",
    "\n",
    "There is a problem with this approach that each link may be different but will point to the same thing. In order to detect these links we have to download every one to verify if they point to the same think. \n",
    "\n",
    "This issue will appear in other approachs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_link_count(href):\n",
    "    wikiart, _ = wiki_db.get_article_by_href(href)\n",
    "    links = wikiart.links()\n",
    "    \n",
    "    link_counter = Counter()\n",
    "    \n",
    "    for href, text in links:\n",
    "        link_counter[href] += 1\n",
    "        \n",
    "    scores_sum = sum(link_counter.values())\n",
    "        \n",
    "    return list(map(lambda a: (a[0], a[1]/scores_sum), link_counter.most_common()))\n",
    "\n",
    "#print_list(simple_link_count(\"Computer\")[:20])\n",
    "#print_data(['Article', 'Score'], simple_link_count(\"Computer\")[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count local wikisyn\n",
    "Compute a wikisyn collection with the local links and score links with them.\n",
    "\n",
    "\n",
    "The wikisyn class computes every href tag text (< a>text< /a>) with their links in order to find link-synonims, that is diferent texts that points to the same links.\n",
    "\n",
    "Variations to this method may be the use of global wikisyn which computes links syns based on other pages references to the links present on the target article improving the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_href_text_count(href):\n",
    "    synDict = WikisynDict()\n",
    "    \n",
    "    wikiart, _ = wiki_db.get_article_by_href(href)\n",
    "    links = wikiart.links()\n",
    "    page_text = wikiart.text()\n",
    "    \n",
    "    #Populate syndict\n",
    "    for href, text in links:\n",
    "        synDict.add_link(href, text)\n",
    "        \n",
    "    #Get matches\n",
    "    links_score = Counter()\n",
    "        \n",
    "    for link_href in synDict.keys():\n",
    "        for l_text, l_score in synDict[link_href]:\n",
    "            matches = re.findall('[^a-zA-Z0-9_]' + re.escape(l_text) + '[^a-zA-Z0-9_]', page_text, re.IGNORECASE)\n",
    "            matches_score = len(matches) * l_score\n",
    "            links_score[link_href] += matches_score\n",
    "\n",
    "    scores_sum = sum(links_score.values())\n",
    "            \n",
    "    #norm_links_scores = list(map(lambda a: (a[0],synDict[a[0]], a[1]/scores_sum), links_score.most_common()))\n",
    "    #norm_links_scores = list(map(lambda a: (a[0], a[1]/scores_sum), links_score.most_common()))\n",
    "    norm_links_scores = list(map(lambda a: (a[0],list(map(lambda b: b[0], synDict[a[0]])), a[1]/scores_sum), links_score.most_common()))\n",
    "    \n",
    "    return norm_links_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE WIKISYN COUNT\n",
      "Article                         Link                                    Score\n",
      "------------------------------  ----------------------------------  ---------\n",
      "Shear_stress                    ['shear stress']                    0.0888889\n",
      "Stress_(physics)                ['shear stress']                    0.0888889\n",
      "Strain_(materials_science)      ['strain']                          0.0740741\n",
      "Physics                         ['physics']                         0.0444444\n",
      "Liquid                          ['liquids', 'liquid']               0.0444444\n",
      "Viscosity                       ['viscosity']                       0.0444444\n",
      "Pressure                        ['pressure']                        0.0296296\n",
      "Phase_(matter)                  ['phases']                          0.0296296\n",
      "Plasticity_(physics)            ['plastic solids', 'solids']        0.0296296\n",
      "Newtonian_fluid                 ['Newtonian fluids']                0.0296296\n",
      "Compressive_stress              ['compressive']                     0.0296296\n",
      "Pitch_(resin)                   ['pitch']                           0.0296296\n",
      "Derivative                      ['derivatives']                     0.0296296\n",
      "Matter                          ['substances', 'matter']            0.0222222\n",
      "Strain_rate                     ['strain rate']                     0.0148148\n",
      "Shear_force                     ['shear force']                     0.0148148\n",
      "Brake_fluid                     ['brake fluid']                     0.0148148\n",
      "Fluid_dynamics                  ['fluid dynamics']                  0.0148148\n",
      "Incompressible                  ['incompressible']                  0.0148148\n",
      "Partial_differential_equations  ['partial differential equations']  0.0148148\n",
      "\n",
      "SIMPLE LINK COUNT\n",
      "Article                             Score\n",
      "------------------------------  ---------\n",
      "Shear_stress                    0.0425532\n",
      "Pressure                        0.0425532\n",
      "Plasticity_(physics)            0.0425532\n",
      "Matter                          0.0425532\n",
      "Liquid                          0.0425532\n",
      "Fluid_mechanics                 0.0425532\n",
      "Viscosity                       0.0425532\n",
      "Viscoelastic                    0.0212766\n",
      "Strain_rate                     0.0212766\n",
      "Strain_(materials_science)      0.0212766\n",
      "Brake_fluid                     0.0212766\n",
      "Fluid_dynamics                  0.0212766\n",
      "Shear_force                     0.0212766\n",
      "Incompressible                  0.0212766\n",
      "Partial_differential_equations  0.0212766\n",
      "University_of_Queensland        0.0212766\n",
      "Mechanical_equilibrium          0.0212766\n",
      "Physics                         0.0212766\n",
      "Tensile_stress                  0.0212766\n",
      "Free_surface                    0.0212766\n",
      "\n",
      "Abstract Links\n",
      "Article                       Score\n",
      "------------------------  ---------\n",
      "Matter                    0.0526316\n",
      "Phase_(matter)            0.0526316\n",
      "Shear_stress              0.0526316\n",
      "Viscoelastic              0.0526316\n",
      "Pitch_(resin)             0.0526316\n",
      "Liquid                    0.0526316\n",
      "Brake_fluid               0.0526316\n",
      "Pitch_drop_experiment     0.0526316\n",
      "Shear_force               0.0526316\n",
      "Incompressible            0.0526316\n",
      "Deformation_(mechanics)   0.0526316\n",
      "Hydraulic_oil             0.0526316\n",
      "Silly_Putty               0.0526316\n",
      "University_of_Queensland  0.0526316\n",
      "Physics                   0.0526316\n",
      "Plasma_(physics)          0.0526316\n",
      "Free_surface              0.0526316\n",
      "Plasticity_(physics)      0.0526316\n",
      "Viscosity                 0.0526316\n"
     ]
    }
   ],
   "source": [
    "#target_article = \"Django_(web_framework)\"\n",
    "target_article = \"Fluid\"\n",
    "\n",
    "print(\"SIMPLE WIKISYN COUNT\")\n",
    "print_data(['Article','Link','Score'],get_href_text_count(target_article)[:20]) \n",
    "print(\"\\nSIMPLE LINK COUNT\")\n",
    "print_data(['Article', 'Score'], simple_link_count(target_article)[:20])\n",
    "print(\"\\nAbstract Links\")\n",
    "print_data(['Article', 'Score'], get_abstract_links(target_article))\n",
    "\n",
    "wiki_db.save()\n",
    "#USE AUTOSUMARIZER IDEA TO FIND IMPORTANT AREAS OF THE TEXT\n",
    "#THE MORE SPREAD ARE SIMILAR WORDS, THE MORE IMPORTANT THEY ARE TO UNDERSTAND THE ENTIRE CONTENT\n",
    "#DIVIDE THE ARTICLE INTO WINDOWS TO CALCULATE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links_score(page):\n",
    "    \"\"\"Function to cross a list of links with a text, setting scores.\"\"\"\n",
    "    \n",
    "    wikiart = get_wiki_article_by_href(page)\n",
    "    \n",
    "    pageid = wikiart.page_id\n",
    "    #Ensure only one href is present\n",
    "    page_links = set([link_href for link_href, link_text in wikiart.links()])\n",
    "    page_text = wikiart.text()\n",
    "    \n",
    "    links_score = dict()\n",
    "    \n",
    "    norm_fact = 0 #norm factor to results sum to one\n",
    "    \n",
    "    #Ensure all page links are present\n",
    "    #check_and_download(page_links)\n",
    "    \n",
    "    #The ideia is to get for each href the texts that use to follow these hrefs in wikipedia articles.\n",
    "    #The more each term appears, it got increased weight.\n",
    "    #Then we try to match each of the terms for each href to the wiki text, applying to each match the correspondent weight.\n",
    "    #The weight stuff is good to avoid cases where the href_text have appeared only once in one article,\n",
    "    #but it is a frequent term in other articles but offen does not mean the original href it pointed to\n",
    "    \n",
    "    for link_href in page_links:\n",
    "        links_score[link_href] = 0\n",
    "        for l_text, l_score in synbeta.get_synoms(link_href): #get_all_wikisyns_by_href(link_href): #wikisyn[link_href]:\n",
    "            matches = re.findall('[^a-zA-Z0-9_]' + re.escape(l_text) + '[^a-zA-Z0-9_]', page_text, re.IGNORECASE)\n",
    "            matches_score = len(matches) * l_score\n",
    "            links_score[link_href] += matches_score\n",
    "            norm_fact += matches_score\n",
    "            \n",
    "    norm_links_score = dict(map(lambda a: [a[0], float(a[1])/norm_fact], links_score.items()))\n",
    "            \n",
    "    return norm_links_score\n",
    "\n",
    "#v_sum = 0\n",
    "#links_score = get_links_score(\"Convolutional neural network\")\n",
    "#for k, v in sorted(links_score.items(), key=lambda a:a[1], reverse=True):\n",
    "    #print(k,synbeta.hrefs[k],v)\n",
    "    #v_sum += v\n",
    "#print v_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_href_text_count(\"MQTT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
